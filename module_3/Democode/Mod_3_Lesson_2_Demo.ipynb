{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q48Z00umUdo6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_3/Democode/Mod_3_Lesson_2_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "QTaoGFZh0Wa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting Model Features and Algorithms\n",
        "\n",
        "In this demonstration, you’ll create a classification model based on a raw dataset and measure the precision and recall. You’ll refine the dataset by selecting and scaling features and assess the impact this has on the performance of the model. You'll also examine how the choice of algorithm can affect the results.\n",
        "\n",
        "This demonstration uses the Bank Marketing dataset.\n",
        "\n",
        "##Context\n",
        "Find the best strategies to improve for the next marketing campaign. How can the financial institution have a greater effectiveness for future marketing campaigns? In order to answer this, we have to analyze the last marketing campaign the bank performed and identify the patterns that will help us find conclusions in order to develop future strategies.\n",
        "\n",
        "##Attribute Information: \n",
        "###Customer Data\n",
        "- Age (numeric)\n",
        "- Job : type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n",
        "- Marital : marital status (categorical: 'divorced', 'married', 'single', 'unknown' ; note: 'divorced' means divorced or widowed)\n",
        "- Education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n",
        "- Default: has credit in default? (categorical: 'no', 'yes', 'unknown')\n",
        "- Housing: has housing loan? (categorical: 'no', 'yes', 'unknown')\n",
        "- Loan: has personal loan? (categorical: 'no', 'yes', 'unknown')\n",
        "\n",
        "###Campaign Data\n",
        "- Contact: contact communication type (categorical:\n",
        "'unknown', 'cellular', 'telephone')\n",
        "- Day: last contact day of the month (numeric)\n",
        "- Month: last contact month of year (categorical: 'jan', 'feb', 'mar',\n",
        "…, 'nov', 'dec')\n",
        "- Duration: last contact duration, in seconds (numeric). Important\n",
        "note: this attribute highly affects the output target (e.g., if\n",
        "duration=0 then y='no'). Yet, the duration is not known before a call\n",
        "is performed. Also, after the end of the call y is obviously known.\n",
        "Thus, this input should only be included for benchmark purposes and\n",
        "should be discarded if the intention is to have a realistic\n",
        "predictive model.\n",
        "- Campaign: number of contacts performed during this campaign and for\n",
        "this client (numeric, includes last contact)\n",
        "- Pdays: number of days that passed by after the client was last\n",
        "contacted from a previous campaign (numeric; -1 means client was not\n",
        "previously contacted)\n",
        "- Previous: number of contacts performed before this campaign and for\n",
        "this client (numeric)\n",
        "- Poutcome: outcome of the previous marketing campaign (categorical:\n",
        "'unknown', 'failure', 'other', 'success')\n",
        "\n",
        "###Target Variable:\n",
        "- Y - has the client subscribed to a term deposit? (binary: 'yes', 'no')\n",
        "\n",
        "##Source\n",
        "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
      ],
      "metadata": {
        "id": "q48Z00umUdo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review the data and build a default model"
      ],
      "metadata": {
        "id": "7OGr7_gIM_qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the marketingdata.csv file\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_3/Democode/marketingdata.csv'"
      ],
      "metadata": {
        "id": "FMvNb7MU0H_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the CSV file\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "marketing"
      ],
      "metadata": {
        "id": "4iMEi04Q2OT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "\n",
        "print(marketing_class)"
      ],
      "metadata": {
        "id": "isZWQgT8Tng7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the class label from the list of features, and replace the categorical variables with numeric dummy values\n",
        "\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "marketing_features = pd.get_dummies(marketing_features)\n",
        "\n",
        "marketing_features"
      ],
      "metadata": {
        "id": "8hX-c-scToYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into test and training datasets, build a K-Nearest Neighbors model, and test the precision, recall, and AUC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)\n",
        "\n",
        "knn_model = KNeighborsClassifier() # Select default hyperparameters (n_neighbors=5)\n",
        "_ = knn_model.fit(features_train, class_train)\n",
        "\n",
        "test_results = knn_model.predict(features_test)\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, test_results, display_labels=['No', 'Yes'])\n",
        "\n",
        "print(f'Precision: {precision_score(class_test, test_results)}')\n",
        "print(f'Recall: {recall_score(class_test, test_results)}\\n')\n",
        "\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "rnyUjStr74vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How good is this model at predicting a positive outcome?\n",
        "\n",
        "*Answer: Poor. The model misses most positive outcomes and reports them as negatives. Additionally, it misclassifies more negative outcomes as positives than it correctly identifies positive outcomes.\n",
        "\n",
        "This is the baseline for further investigation.*"
      ],
      "metadata": {
        "id": "SUtUYkNgJr2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform a SHAP analysis to examine which features have the most effect on the predictions"
      ],
      "metadata": {
        "id": "qDrkq4Gb4ETJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the SHAP module\n",
        "\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "EW6WlAcN8frf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SHAP explainer to analyze predictions made using the model\n",
        "# NOTE: This step takes 5 minutes to run\n",
        "\n",
        "import shap\n",
        "import random\n",
        "\n",
        "items = random.sample(list(features_train.index), 50) # The analysis is restricted to the 50 random observations to save time\n",
        "explainer_train = features_train[features_train.index.isin(items)]\n",
        "\n",
        "explainer = shap.Explainer(knn_model.predict, explainer_train)\n",
        "values = explainer(explainer_train)"
      ],
      "metadata": {
        "id": "rxD-fxOv4YPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results as a summary plot\n",
        "\n",
        "shap.summary_plot(shap_values=values, features=explainer_train, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "tm6hl4LxxuOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The violin plot indicateshow the features are correlated with the predictions\n",
        "\n",
        "shap.summary_plot(shap_values=values, features=explainer_train, plot_type=\"violin\")"
      ],
      "metadata": {
        "id": "Q_pYkO695VRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "What does this analysis indicate?\n",
        "\n",
        "*Answer: Most of the features have little to no bearing on the predictions. It would appear that the most important features are duration, balance, day, age, and pdays, although the corellations are on the 'blue' side (low). The Day feature is a surprise - why should making contact on a  day of the month be important? Further analysis is necessary to verify these findings*"
      ],
      "metadata": {
        "id": "FnNS_1giAJ8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform a multivariate search to find the most important features for the model"
      ],
      "metadata": {
        "id": "yOTzxosFSzbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a forward selection search using the SelectKBest function\n",
        "# NOTE: This step takes 5 or 6 minutes to run\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Iterate over the best models with different sized feature sets \n",
        "# and calculate the precision and recall of each model\n",
        "\n",
        "pr_scores = []\n",
        "rc_scores = []\n",
        "for k in range(1, len(features_train.columns)-1):\n",
        "    features_selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    features_selector = features_selector.fit(features_train, class_train)\n",
        "    print(features_selector.get_feature_names_out())\n",
        "    transformed_train = features_selector.transform(features_train)\n",
        "    transformed_test = features_selector.transform(features_test)\n",
        "    model = KNeighborsClassifier()\n",
        "    model.fit(transformed_train, class_train)\n",
        "    predictions = model.predict(transformed_test)\n",
        "    pr_score = metrics.precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "    pr_scores.append(pr_score)\n",
        "    rc_score = metrics.recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "    rc_scores.append(rc_score)"
      ],
      "metadata": {
        "id": "-Tq_fmj19YAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(40, 10))\n",
        "plt.plot(range(1, len(features_train.columns)-1), pr_scores, label='Precision')\n",
        "plt.plot(range(1, len(features_train.columns)-1), rc_scores, label='Recall')\n",
        "plt.xlabel('\\nBest K Features', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.xticks(range(1, len(features_train.columns)-1))\n",
        "plt.ylabel('Precision/Recall Scores', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WN74SjsMBG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best precision score to minimize the false positive rate \n",
        "# (use recall to minimize the false negative rate)\n",
        "\n",
        "best_score = max(pr_scores)\n",
        "num_features = np.where(pr_scores == best_score)[0][0]\n",
        "features_selector = SelectKBest(score_func=f_classif, k=num_features+1)\n",
        "features_selector = features_selector.fit(features_train, class_train)\n",
        "print(f'Best features: {features_selector.get_feature_names_out()}')"
      ],
      "metadata": {
        "id": "s3i2-VDGVWw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How do these findings compare to the SHAP analysis?\n",
        "\n",
        "*Answer: Day has dropped out of the list, but certain types of job, whether the customer owns their own home, the method of contact, the month, and the outcome now appear to be important*"
      ],
      "metadata": {
        "id": "fsrpC6KJWAJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build and test a model using the *'best'* set of features "
      ],
      "metadata": {
        "id": "V4BKuCwgWRKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_features_train = features_train[['duration', 'pdays', 'previous', 'job_retired', 'job_student', 'housing_no', 'housing_yes', 'contact_cellular', 'contact_unknown', 'month_dec', \\\n",
        "                                      'month_mar', 'month_may', 'month_oct', 'month_sep', 'poutcome_success', 'poutcome_unknown']]\n",
        "\n",
        "best_features_test = features_test[['duration', 'pdays', 'previous', 'job_retired', 'job_student', 'housing_no', 'housing_yes', 'contact_cellular', 'contact_unknown', 'month_dec', \\\n",
        "                                    'month_mar', 'month_may', 'month_oct', 'month_sep', 'poutcome_success', 'poutcome_unknown']]"
      ],
      "metadata": {
        "id": "NpS7Q6pyBoq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsClassifier() \n",
        "_ = knn_model.fit(best_features_train, class_train)\n",
        "\n",
        "test_results = knn_model.predict(best_features_test)\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(class_test, test_results)}')\n",
        "print(f'Recall: {recall_score(class_test, test_results)}\\n')\n",
        "\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "sBXxu2AxZ5XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Has the model improved?\n",
        "\n",
        "*Answer: Slightly. Precision is better, but still not good. Recall remains poor.*"
      ],
      "metadata": {
        "id": "3SB42pTvarwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Examine how scaling affects the choice of features"
      ],
      "metadata": {
        "id": "M0NfwG-WWmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numeric features have varying scales, so it may be worth standardizing them to see how this impacts the model"
      ],
      "metadata": {
        "id": "oJSxSLgnbohp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Return to the original dataset\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "print(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "print(marketing_features)\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "2yKugy5NDECB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline to perform encoding of the categorical features and scaling of the numeric features\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import set_config\n",
        "\n",
        "numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays',\t'previous']\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_preprocessor = Pipeline([('categorical_encoder', OneHotEncoder())])\n",
        "\n",
        "# Clean and scale numeric features\n",
        "numeric_preprocessor = Pipeline([('replace_nan', SimpleImputer(missing_values=np.nan, strategy='mean')),\\\n",
        "                                 ('numeric_scaler', StandardScaler())])\n",
        "\n",
        "# Apply transformations to categorical and numeric columns as appropriate\n",
        "pipeline_preprocessor = \\\n",
        "    ColumnTransformer([('numeric_preprocessor', numeric_preprocessor, numeric_features), \\\n",
        "                       ('categorical_preprocessor', categorical_preprocessor, categorical_features)])\n",
        "\n",
        "# Create the pipeline and fit a K-Nearest Neighbours model\n",
        "pipe = Pipeline([('preprocessor', pipeline_preprocessor),\n",
        "                 ('estimator', KNeighborsClassifier())])\n",
        "\n",
        "pipe.fit(features_train, class_train)\n",
        "\n",
        "# Display the details of the pipe\n",
        "set_config(display=\"diagram\")\n",
        "pipe"
      ],
      "metadata": {
        "id": "zpSMV1_YdC-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the names of the features generated by the pipeline\n",
        "\n",
        "numeric_feature_names = pipe['preprocessor'].transformers_[0][1]['numeric_scaler'].get_feature_names_out(numeric_features)\n",
        "print(f'Numeric features: {numeric_feature_names}\\n')\n",
        "\n",
        "categorical_feature_names = pipe['preprocessor'].transformers_[1][1]['categorical_encoder'].get_feature_names_out(categorical_features)\n",
        "print(f'Categorical features: {categorical_feature_names}')"
      ],
      "metadata": {
        "id": "qAufWhpcDPMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Make test predictions\n",
        "predictions = pipe.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = metrics.precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = metrics.recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, predictions)"
      ],
      "metadata": {
        "id": "UTvo1W6fpz6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Has the model improved?\n",
        "\n",
        "*Answer: Precision and recall are better. AUC is still low.*"
      ],
      "metadata": {
        "id": "Z9beTff8g6H8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform SHAP analysis on the new model"
      ],
      "metadata": {
        "id": "GykHEkA7hiLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline again without the estimator to get the transformed training data\n",
        "\n",
        "transformed_features_train = pd.DataFrame(pipe[0].transform(features_train))\n",
        "transformed_features_train"
      ],
      "metadata": {
        "id": "4InHL6W8ieud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The feature names have been lost, so reinstate them from the lists seen earlier\n",
        "\n",
        "new_names = np.append(numeric_feature_names, categorical_feature_names)\n",
        "transformed_features_train.columns = new_names\n",
        "transformed_features_train"
      ],
      "metadata": {
        "id": "5d8v3o1d2qt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform SHAP analysis over the transformed training data\n",
        "# NOTE: Allow 5 or 6 minutes for this step to complete\n",
        "# NOTE 2: Ignore the warnings about the classifier not being fitted with feature names\n",
        "\n",
        "import shap\n",
        "import random\n",
        "\n",
        "items = random.sample(list(transformed_features_train.index), 50) # As before, the analysis is restricted to the 50 random observations to save time\n",
        "explainer_train = transformed_features_train[transformed_features_train.index.isin(items)]\n",
        "\n",
        "explainer = shap.Explainer(pipe['estimator'].predict, explainer_train) # Perform the analysis using the 'estimator' object from the pipeline\n",
        "values = explainer(explainer_train)"
      ],
      "metadata": {
        "id": "d2QA4B6UePN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "\n",
        "shap.summary_plot(shap_values=values, features=explainer_train, plot_type=\"bar\")\n",
        "shap.summary_plot(shap_values=values, features=explainer_train, plot_type=\"violin\")"
      ],
      "metadata": {
        "id": "D8WSmZXl-G7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "What does this analysis tell us?\n",
        "\n",
        "*Answer: Scaling the numeric features results in more features having a greater influence in the model predictions. Day still seems to be relevant!*"
      ],
      "metadata": {
        "id": "mSScsEOJjctq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform multivariate forward selection with the new model and assess whether this change improves predictions"
      ],
      "metadata": {
        "id": "AFePv6QmkWbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the SequentialFeatureSelector to find the best set of features for the model\n",
        "# NOTE: This step takes approximately 6 minutes to run\n",
        "\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "features_to_select = 5\n",
        "\n",
        "sfs_forward = SequentialFeatureSelector(pipe['estimator'], n_features_to_select=features_to_select, direction=\"forward\")\n",
        "sfs_forward.fit(transformed_features_train, class_train)\n",
        "\n",
        "print(f\"Features selected by forward sequential selection: {sfs_forward.get_feature_names_out()}\")"
      ],
      "metadata": {
        "id": "PTHgNb07kjnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the test data and rename the columns\n",
        "\n",
        "transformed_features_test = pd.DataFrame(pipe[0].transform(features_test))\n",
        "transformed_features_test.columns = new_names\n",
        "transformed_features_test"
      ],
      "metadata": {
        "id": "h01pFQpXpUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a new model using only the selected features\n",
        "\n",
        "reduced_features_train = transformed_features_train[sfs_forward.get_feature_names_out()]\n",
        "reduced_features_test = transformed_features_test[sfs_forward.get_feature_names_out()]\n",
        "\n",
        "knn_model = KNeighborsClassifier() # Create a new classifier\n",
        "_ = knn_model.fit(reduced_features_train, class_train)\n",
        "\n",
        "test_results = knn_model.predict(reduced_features_test)\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, test_results, display_labels=['No', 'Yes'])\n",
        "\n",
        "print(f'Precision: {precision_score(class_test, test_results)}')\n",
        "print(f'Recall: {recall_score(class_test, test_results)}\\n')\n",
        "\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "2ExyHiC-mnVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How does this model fare?\n",
        "\n",
        "*Answer: Focussing on a smaller number of columns reduced the precision and recall, so this might not be the best strategy in this case*"
      ],
      "metadata": {
        "id": "-5A3Vl2tKBfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform detailed multivariate forward selection and select the features more carefully"
      ],
      "metadata": {
        "id": "AV_FxhMxMBzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform another forward selection search using the SelectKBest function and evaluate the best mix of features for precision and recall\n",
        "# NOTE: This step takes 5 or 6 minutes to run\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Iterate over the best models with different sized feature sets \n",
        "# and calculate the precision and recall of each model\n",
        "\n",
        "pr_scores = []\n",
        "rc_scores = []\n",
        "for k in range(1, len(transformed_features_train.columns)-1):\n",
        "    features_selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    features_selector = features_selector.fit(transformed_features_train, class_train)\n",
        "    print(features_selector.get_feature_names_out())\n",
        "    transformed_train = features_selector.transform(transformed_features_train)\n",
        "    transformed_test = features_selector.transform(transformed_features_test)\n",
        "    model = KNeighborsClassifier()\n",
        "    model.fit(transformed_train, class_train)\n",
        "    predictions = model.predict(transformed_test)\n",
        "    pr_score = metrics.precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "    pr_scores.append(pr_score)\n",
        "    rc_score = metrics.recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "    rc_scores.append(rc_score)\n",
        "\n",
        "# Plot the results\n",
        "\n",
        "plt.figure(figsize=(40, 10))\n",
        "plt.plot(range(1, len(transformed_features_train.columns)-1), pr_scores, label='Precision')\n",
        "plt.plot(range(1, len(transformed_features_train.columns)-1), rc_scores, label='Recall')\n",
        "plt.xlabel('\\nBest K Features', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.xticks(range(1, len(transformed_features_train.columns)-1))\n",
        "plt.ylabel('Precision/Recall Scores', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NiBZgIX2KjFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the features for the best precision score to minimize the false positive rate \n",
        "# (use recall to minimize the false negative rate)\n",
        "\n",
        "best_score = max(pr_scores)\n",
        "num_features = np.where(pr_scores == best_score)[0][0]\n",
        "features_selector = SelectKBest(score_func=f_classif, k=num_features+1)\n",
        "features_selector = features_selector.fit(transformed_features_train, class_train)\n",
        "print(f'Best features: {features_selector.get_feature_names_out()}')"
      ],
      "metadata": {
        "id": "xj0hvy9BL6li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build another new model using only the selected features\n",
        "\n",
        "reduced_features_train = transformed_features_train[features_selector.get_feature_names_out()]\n",
        "reduced_features_test = transformed_features_test[features_selector.get_feature_names_out()]\n",
        "\n",
        "knn_model = KNeighborsClassifier() # Create a new classifier\n",
        "_ = knn_model.fit(reduced_features_train, class_train)\n",
        "\n",
        "test_results = knn_model.predict(reduced_features_test)\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, test_results, display_labels=['No', 'Yes'])\n",
        "\n",
        "print(f'Precision: {precision_score(class_test, test_results)}')\n",
        "print(f'Recall: {recall_score(class_test, test_results)}\\n')\n",
        "\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "U5dSGKPGOYqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Is this model an improvement?\n",
        "\n",
        "*Answer: Selecting features based on model precision yields an improvement, and AUC is increasing, but still not as good as selecting every feature.*"
      ],
      "metadata": {
        "id": "cLCKDB5fPMDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try using feature extraction as an alternative strategy"
      ],
      "metadata": {
        "id": "UOqjdw0tPj8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Return to the original dataset again\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "print(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "marketing_features = pd.get_dummies(marketing_features)\n",
        "print(marketing_features)\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "C8MqpgbdPxd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Perform PCA analysis\n",
        "pca = PCA()\n",
        "pca.fit(features_train)\n",
        "\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "amMqxDwiQynO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10,10))\n",
        "x = np.arange(1, len(pca.explained_variance_)+1)\n",
        "plt.bar(x, pca.explained_variance_ratio_)\n",
        "plt.xlabel('Principal Components', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.ylabel('Proportion of Explained Variances', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pjxHkaqlReOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Which component(s) account for the most variance?\n",
        "\n",
        "*Answer: Component 1 accounts for 99.2% of the variance. This component dwarfs the variance of the other components. Try building a model with this single component.*"
      ],
      "metadata": {
        "id": "_VT75MnzRroR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct another model using the first principal component\n",
        "\n",
        "pca_data = pd.DataFrame(pca.transform(marketing_features))\n",
        "first_component_data = pca_data.iloc[:, 0:1]\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "pca_train, pca_test, pca_class_train, pca_class_test = train_test_split(first_component_data, marketing_class, test_size=0.33, random_state=13)\n",
        "\n",
        "# Build the model\n",
        "pca_knn_model = KNeighborsClassifier()\n",
        "pca_knn_model.fit(pca_train, pca_class_train)\n",
        "predictions = pca_knn_model.predict(pca_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(pca_class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(pca_class_test, test_results)"
      ],
      "metadata": {
        "id": "eJtQr3aaSUtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Is this model an improvement?\n",
        "\n",
        "*Answer: Precision and Recall have dropped, although AUC is climbing (very slowly). Compacting the predictive power into a single component was probably optimistic.*"
      ],
      "metadata": {
        "id": "yL3TBILdVY_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try again with the first three principal components\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Return to the original dataset again\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "print(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "marketing_features = pd.get_dummies(marketing_features)\n",
        "print(marketing_features)\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "HaNruPHMBubi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_data = pd.DataFrame(pca.transform(marketing_features))\n",
        "first_component_data = pca_data.iloc[:, 0:3]\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "pca_train, pca_test, pca_class_train, pca_class_test = train_test_split(first_component_data, marketing_class, test_size=0.33, random_state=13)\n",
        "\n",
        "# Build the model\n",
        "pca_knn_model = KNeighborsClassifier()\n",
        "pca_knn_model.fit(pca_train, pca_class_train)\n",
        "predictions = pca_knn_model.predict(pca_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(pca_class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(pca_class_test, test_results)"
      ],
      "metadata": {
        "id": "34hw_sQzAM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Is this model an improvement?\n",
        "\n",
        "*Answer: Precision and Recall have improved. Maybe there is more information in the second and third components than is alluded to by their variance.*"
      ],
      "metadata": {
        "id": "ddawuhNlAdzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Incorporate scaling with PCA"
      ],
      "metadata": {
        "id": "tvN1USMjCIz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Return to the original dataset again\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "print(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "print(marketing_features)\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "JJQYd1ZnCR7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try scaling before performing PCA.\n",
        "# Construct a new pipeline that includes feature extraction\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import set_config\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays',\t'previous']\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_preprocessor = Pipeline([('categorical_encoder', OneHotEncoder())])\n",
        "\n",
        "# Clean and scale numeric features\n",
        "numeric_preprocessor = Pipeline([('replace_nan', SimpleImputer(missing_values=np.nan, strategy='mean')),\\\n",
        "                                 ('numeric_scaler', StandardScaler())])\n",
        "\n",
        "# Apply transformations to categorical and numeric columns as appropriate\n",
        "pipeline_preprocessor = \\\n",
        "    ColumnTransformer([('numeric_preprocessor', numeric_preprocessor, numeric_features), \\\n",
        "                       ('categorical_preprocessor', categorical_preprocessor, categorical_features)])\n",
        "\n",
        "# Create the pipeline with PCA and fit a K-Nearest Neighbors model\n",
        "pca_pipe = Pipeline([('preprocessor', pipeline_preprocessor),\n",
        "                     ('extractor', PCA(n_components=1)), # Only generate the first PCA component\n",
        "                     ('estimator', KNeighborsClassifier())])\n",
        "\n",
        "pca_pipe.fit(features_train, class_train)\n",
        "\n",
        "# Display the details of the pipe\n",
        "set_config(display=\"diagram\")\n",
        "pca_pipe"
      ],
      "metadata": {
        "id": "oTtOT7Eu_l7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "\n",
        "predictions = pca_pipe.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "JnU4uWqXF8A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How does this model compare to previously?\n",
        "\n",
        "*Answer: Precision and Recall are back to where they were with a single component without scaling.*"
      ],
      "metadata": {
        "id": "i78G0ltDK2kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try again with three principal components\n",
        "\n",
        "pca_pipe[1].set_params(**{'n_components': 3})\n",
        "\n",
        "pca_pipe.fit(features_train, class_train)\n",
        "predictions = pca_pipe.predict(features_test)\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, test_results)"
      ],
      "metadata": {
        "id": "LKMdcns2I5lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How about now?\n",
        "\n",
        "*Answer: Precision and Recall have improved, but are still only comparable to the model without scaling.*"
      ],
      "metadata": {
        "id": "tN_qiLtCLJRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try multivariate feature selection to ascertain how selecting multiple PCA components affects the model\n",
        "# NOTE: This step takes 5 or 6 minutes to run\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Run the pipeline again to generate all PCA components\n",
        "pca_pipe[1].set_params(**{'n_components': None})\n",
        "pca_pipe.fit(features_train, class_train)\n",
        "\n",
        "# Generate the transformed data using the pipeline\n",
        "pca_data = pd.DataFrame(pca_pipe[:-1].transform(marketing_features))\n",
        "\n",
        "# Rename the columns returned by PCA analysis - the existing column names are numeric which can cause problems. Prepend each name with an 'x'\n",
        "pca_column_names = [f'x%d' % i for i in pca_data.columns]\n",
        "pca_data = pca_data.set_axis(pca_column_names, axis=1)\n",
        "\n",
        "# Split the data into training and test datasets\n",
        "pca_train, pca_test, pca_class_train, pca_class_test = train_test_split(pca_data, marketing_class, test_size=0.33, random_state=13)\n",
        "num_rows, num_cols = pca_data.shape\n",
        "\n",
        "# Iterate over the best models with different sized feature sets \n",
        "# and calculate the precision and recall of each model\n",
        "\n",
        "pr_scores = []\n",
        "rc_scores = []\n",
        "for k in range(1, num_cols-1):\n",
        "    features_selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    features_selector = features_selector.fit(pca_train, pca_class_train)\n",
        "    print(features_selector.get_feature_names_out())\n",
        "    transformed_train = features_selector.transform(pca_train)\n",
        "    transformed_test = features_selector.transform(pca_test)\n",
        "    model = KNeighborsClassifier()\n",
        "    model.fit(transformed_train, class_train)\n",
        "    predictions = model.predict(transformed_test)\n",
        "    pr_score = metrics.precision_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "    pr_scores.append(pr_score)\n",
        "    rc_score = metrics.recall_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "    rc_scores.append(rc_score)\n",
        "\n",
        "# Plot the results\n",
        "\n",
        "plt.figure(figsize=(40, 10))\n",
        "plt.plot(range(1, num_cols-1), pr_scores, label='Precision')\n",
        "plt.plot(range(1, num_cols-1), rc_scores, label='Recall')\n",
        "plt.xlabel('\\nBest K Features', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.xticks(range(1, num_cols-1))\n",
        "plt.ylabel('Precision/Recall Scores', fontdict={'family': 'serif','color':  'darkred','weight': 'normal','size': 28})\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hFdWyFJgULs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the features for the best precision score to minimize the false positive rate \n",
        "# (use recall to minimize the false negative rate)\n",
        "\n",
        "best_score = max(pr_scores)\n",
        "num_features = np.where(pr_scores == best_score)[0][0]\n",
        "features_selector = SelectKBest(score_func=f_classif, k=num_features+1)\n",
        "features_selector = features_selector.fit(pca_train, pca_class_train)\n",
        "print(f'Best features: {features_selector.get_feature_names_out()}')"
      ],
      "metadata": {
        "id": "28xyc5SlbrSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct another model using the highlighted principal components\n",
        "\n",
        "pca_knn_model = KNeighborsClassifier()\n",
        "pca_knn_model.fit(pca_train[features_selector.get_feature_names_out()], pca_class_train)\n",
        "predictions = pca_knn_model.predict(pca_test[features_selector.get_feature_names_out()])\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = metrics.precision_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = metrics.recall_score(pca_class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(pca_class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(pca_class_test, test_results)"
      ],
      "metadata": {
        "id": "2VsLFWzhfNqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Is this an improvement?\n",
        "\n",
        "*Answer: Precision and Recall have improved signifcantly. They are back to where they were prior to PCA!*"
      ],
      "metadata": {
        "id": "HxDXFutAXFCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare PCA to t-SNE"
      ],
      "metadata": {
        "id": "l4GR50r6YS1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Only do this part if time allows, otherwise go straight to UMAP\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Return to the original dataset again\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "items = random.sample(list(marketing.index), 10000) # The analysis is restricted to the 10000 random observations to save time; TSNE analysis is very resource intensive\n",
        "marketing_subset = marketing[marketing.index.isin(items)]\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing_subset['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "print(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features and encode the categorical features\n",
        "marketing_features = marketing_subset.drop(['y'], axis=1)\n",
        "marketing_features = pd.get_dummies(marketing_features)\n",
        "print(marketing_features)"
      ],
      "metadata": {
        "id": "SRleWttCf0tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: t-SNE is not compatible with sklearn pipelines\n",
        "# NOTE 2: Allow 7 minutes for this step\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Perform t-SNE analysis\n",
        "tsne = TSNE(n_components=3) # Reduce the dataset to 3 dimensions\n",
        "transformed_data = tsne.fit_transform(marketing_features)\n",
        "tsne_features = transformed_data[:, 0:3]\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "tsne_features_train, tsne_features_test, tsne_class_train, tsne_class_test = train_test_split(tsne_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "aDPTKH5WjYAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Build a model using the new dataset\n",
        "\n",
        "tsne_knn_model = KNeighborsClassifier()\n",
        "tsne_knn_model.fit(tsne_features_train, tsne_class_train)\n",
        "predictions = tsne_knn_model.predict(tsne_features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(tsne_class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(tsne_class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(tsne_class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(tsne_class_test, predictions)"
      ],
      "metadata": {
        "id": "ZJzIP08eksEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How does performance compare to PCA?\n",
        "\n",
        "*Answer: Precision and Recall are not as good. Could possibly tune by experimenting with the perplexity and learning rate, but it is time-consuming*"
      ],
      "metadata": {
        "id": "EBxAPGc15SyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try UMAP"
      ],
      "metadata": {
        "id": "dEKmuT-n5phZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "cKotj6Uw6g7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Return to the original dataset again\n",
        "marketing = pd.read_csv(\"marketingdata.csv\", sep=';')\n",
        "print(f'{marketing.info()}\\n\\n')\n",
        "\n",
        "# Separate the class variable ('Y') from the features and convert the label (yes/no) into a numeric value (1/0)\n",
        "marketing_class = marketing['y']\n",
        "cat_encoder = LabelEncoder().fit(marketing_class)\n",
        "marketing_class = cat_encoder.transform(marketing_class)\n",
        "\n",
        "# Remove the class label from the list of features\n",
        "marketing_features = marketing.drop(['y'], axis=1)\n",
        "\n",
        "# Split the data into test and training datasets\n",
        "features_train, features_test, class_train, class_test = train_test_split(marketing_features, marketing_class, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "B8F_6uD35vlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline to perform encoding of the categorical features and scaling of the numeric features\n",
        "\n",
        "import umap.umap_ as umap\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import set_config\n",
        "\n",
        "numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays',\t'previous']\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_preprocessor = Pipeline([('categorical_encoder', OneHotEncoder())])\n",
        "\n",
        "# Clean and scale numeric features\n",
        "numeric_preprocessor = Pipeline([('replace_nan', SimpleImputer(missing_values=np.nan, strategy='mean')),\\\n",
        "                                 ('numeric_scaler', StandardScaler())])\n",
        "\n",
        "# Apply transformations to categorical and numeric columns as appropriate\n",
        "pipeline_preprocessor = \\\n",
        "    ColumnTransformer([('numeric_preprocessor', numeric_preprocessor, numeric_features), \\\n",
        "                       ('categorical_preprocessor', categorical_preprocessor, categorical_features)])\n",
        "\n",
        "# Create the pipeline and fit a K-Nearest Neighbours model\n",
        "umap_pipe = Pipeline([('preprocessor', pipeline_preprocessor),\n",
        "                      ('reducer', umap.UMAP(n_components=5)),\n",
        "                      ('estimator', KNeighborsClassifier())])\n",
        "\n",
        "umap_pipe.fit(features_train, class_train)\n",
        "\n",
        "# Display the details of the pipe\n",
        "set_config(display=\"diagram\")\n",
        "umap_pipe"
      ],
      "metadata": {
        "id": "UJTBROzlK4qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Make predictions\n",
        "predictions = umap_pipe.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, predictions)"
      ],
      "metadata": {
        "id": "GnybUVRl8Rv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How does performance compare to PCA and t-SNE?\n",
        "\n",
        "*Answer: Precision and Recall are on a par (possibly slightly better) with the scaled version without feature extraction. They are better with than PCA and much better than with t-SNE.*"
      ],
      "metadata": {
        "id": "3IABwnNkJy7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try different algorithms - Logistic Regression and Random Forest"
      ],
      "metadata": {
        "id": "aRu5qycqKV8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Change the K-Nearest Neigbors estimator in the umap pipeline for a Logistic Regression estimator\n",
        "\n",
        "umap_pipe.set_params(**{'estimator': LogisticRegression(max_iter=1000, solver=\"lbfgs\", tol=1e-3)})\n",
        "umap_pipe"
      ],
      "metadata": {
        "id": "psYRya1zcnGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Fit the model \n",
        "umap_pipe.fit(features_train, class_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = umap_pipe.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, predictions)"
      ],
      "metadata": {
        "id": "TqjdGtD7KU43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "How does this model compare with those seen so far?\n",
        "\n",
        "*Answer: This model is the best yet for precision and is OK for recall. This is a significant improvement on the initial model which had a precision of 48.7% and a recall of 27.1%*"
      ],
      "metadata": {
        "id": "kFPmZSC6mgJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define a new pipeline that selects all features and doesn't scale the numeric data (Tree models are not sensitive to scaling)\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_preprocessor = Pipeline([('categorical_encoder', OneHotEncoder())])\n",
        "\n",
        "# Apply transformations to categorical and numeric columns as appropriate\n",
        "pipeline_preprocessor = \\\n",
        "    ColumnTransformer([('categorical_preprocessor', categorical_preprocessor, categorical_features)])\n",
        "\n",
        "# Create the pipeline fit a Random Forest model\n",
        "forest_pipe = Pipeline([('preprocessor', pipeline_preprocessor),\n",
        "                        ('estimator', RandomForestClassifier())])\n",
        "\n",
        "forest_pipe.fit(features_train, class_train)"
      ],
      "metadata": {
        "id": "gJEMxPY0nu7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Make predictions\n",
        "predictions = forest_pipe.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, predictions)"
      ],
      "metadata": {
        "id": "XKSPWCdtpxPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Stacking to Reduce Bias and Variance across Multiple Algorithms"
      ],
      "metadata": {
        "id": "6oPdtiixrVZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a stack with pipelines for Random Forest, UMAP, and Naive Bayes estimators. Use Logistic Regression to aggregate the results\n",
        "# NOTE: Allow 5 minutes to run this step\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Create a new pipeline for K-Nearest Neighbors that generates dummy values for categorical data and scale the numeric features\n",
        "numeric_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays',\t'previous']\n",
        "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_preprocessor = Pipeline([('categorical_encoder', OneHotEncoder())])\n",
        "\n",
        "# Clean and scale numeric features\n",
        "numeric_preprocessor = Pipeline([('replace_nan', SimpleImputer(missing_values=np.nan, strategy='mean')),\\\n",
        "                                 ('numeric_scaler', StandardScaler())])\n",
        "\n",
        "# Apply transformations to categorical and numeric columns as appropriate\n",
        "pipeline_preprocessor = \\\n",
        "    ColumnTransformer([('numeric_preprocessor', numeric_preprocessor, numeric_features), \\\n",
        "                       ('categorical_preprocessor', categorical_preprocessor, categorical_features)])\n",
        "\n",
        "# Create a similar pipeline for Naive Bayes\n",
        "nb_pipe =  Pipeline([('preprocessor', pipeline_preprocessor),\n",
        "                     ('estimator', GaussianNB())])\n",
        "\n",
        "# Create a stack comprising the random forest pipeline from the previous tasks and the UMAP and Naive Bayes pipelines.\n",
        "# The default aggregator at the top of the stack uses Logistic Regression\n",
        "\n",
        "estimators = [\n",
        "    (\"rf\", forest_pipe),\n",
        "    (\"nb\", nb_pipe),\n",
        "    (\"umap\", umap_pipe)\n",
        "]\n",
        "\n",
        "sc = StackingClassifier(estimators=estimators)\n",
        "\n",
        "sc.fit(features_train, class_train)"
      ],
      "metadata": {
        "id": "7JITyUQYrTH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the stacked model\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "\n",
        "# Make predictions\n",
        "predictions = sc.predict(features_test)\n",
        "\n",
        "# Check the precision and recall\n",
        "pr_score = precision_score(class_test, predictions, zero_division=0, average='macro')\n",
        "rc_score = recall_score(class_test, predictions, zero_division=0, average='macro')\n",
        "\n",
        "print(f'Precision is {pr_score}\\nRecall is {rc_score}')\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(class_test, predictions, display_labels=['No', 'Yes'])\n",
        "_ = RocCurveDisplay.from_predictions(class_test, predictions)"
      ],
      "metadata": {
        "id": "0j8p-d_gszw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "Feature selection, feature extraction, scaling, and algorithm choice are all important factors to consider when building a machine learning model. It is vitally important that you test and verify the decisions you make. Additionally, you should always consider the possibility of ensemble methods, such as stacking, to help reduce the shortcomings of a particular algorithm when clasifying your data.\n",
        "\n",
        "In this demonstration, the focus has been on raising the Precision and Recall, with scant regard to AUC. This is partly due to the dataset, which has a highly imbalanced distribution for the Yes/No labels (No outnumbers Yes by a factor exceeding 25 times). You'll examine how to address this issue in a later lesson in this module.\n",
        "\n",
        "It is always important to understand the limitations of a model. Different feature selection and extraction strategies can have an impact (positive and negative) on the model, as can the choice of algorithm."
      ],
      "metadata": {
        "id": "ifvVUr3AlZwv"
      }
    }
  ]
}