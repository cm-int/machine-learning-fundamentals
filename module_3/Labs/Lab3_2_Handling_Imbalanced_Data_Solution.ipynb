{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RLThTHGMpRXH",
        "sQMO2fH96T_B"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_3/Labs/Lab3_2_Handling_Imbalanced_Data_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "GnXIeSXSesog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3.2: Handling Imbalanced Data\n",
        "\n",
        "In this lab, you'll perform the following tasks:\n",
        "\n",
        "1. Build a Random Forest model to classify an imbalanced dataset without making any modifications.\n",
        "1. Examine the results and evaluate the performance using appropriate metrics.\n",
        "1. Use sampling to balance the dataset and rebuild and retest the model.\n",
        "1. Use bagging with sampling and rebuild and retest the model.\n",
        "1. Use boosting with sampling and rebuild and restest the model.\n",
        "1. Calibrate the model and restest it.\n",
        "1. Build another model using the original imbalanced dataset, then calibrate and evaluate the model.\n",
        "1. Combine models using a VotingClassifier and evaluate the results.\n",
        "\n",
        "## Scenario\n",
        "\n",
        "Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year and exerting a significant financial burden on the economy.\n",
        "\n",
        "Complications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure for diabetes, strategies like losing weight, eating healthily, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.\n",
        "\n",
        "The Behavioral Risk Factor Surveillance System (BRFSS) is a system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "\n",
        "Input variables:\n",
        "* HighBP. 0=no high BP, 1=high BP\n",
        "* HighChol. 0=no high cholesterol, 1=high cholesterol\n",
        "* CholCheck. Has the pateint had a cholesterol check in the last 5 years? 0=no, 1=yes\n",
        "* BMI. Body Mass Index\n",
        "* Smoker. Has the patient smoked at least 100 cigarettes in their entire life? [Note: 5 packs = 100 cigarettes] 0=no, 1=yes\n",
        "* Stroke. Has the patient ever had a stroke? 0=no, 1=yes\n",
        "* HeartDiseaseorAttack. Does the patient have coronary heart disease (CHD) or myocardial infarction (MI)? 0=no, 1=yes\n",
        "* PhysActivity. Has the patient performed any physical activity in past 30 days, not including job? 0=no, 1=yes\n",
        "* Fruits. Does the patient consume fruit 1 or more times per day? 0=no, 1=yes\n",
        "* Veggies. Does the patient consume vegetables 1 or more times per day? 0=no, 1=yes\n",
        "* HvyAlcoholConsump. Is the patient a heavy drinkerer (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)? 0=no, 1=yes\n",
        "* AnyHealthcare. Does the patient have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc? 0=no, 1=yes\n",
        "* NoDocbcCost. Was there a time in the past 12 months when the patient needed to see a doctor but could not because of cost? 0=no, 1=yes\n",
        "* GenHlth. Would the pateint say that in general their health is: scale 1-5 1=excellent, 2=very good, 3=good, 4=fair, 5=poor\n",
        "* MentHlth. Including stress, depression, and problems with emotions, for how many days during the past 30 days was the patient's mental health not good? scale 1-30 days\n",
        "* PhysHlth. Inlcuding physical illness and injury, for how many days during the past 30 days was the patient's physical health not good? scale 1-30 days\n",
        "* DiffWalk. Does the patient have serious difficulty walking or climbing stairs? 0=no, 1=yes\n",
        "* Sex. 0=female, 1=male\n",
        "* Age. 13-level age category (_AGEG5YR see codebook) 1=18-24, 9=60-64, 13=80 or older\n",
        "* Education. Education level (EDUCA see codebook) scale 1-6 1=Never attended school or only kindergarten, 2=Grades 1 through 8 (Elementary), 3=Grades 9 through 11 (Some high school), 4=Grade 12 or GED (High school graduate), 5=College 1 year to 3 years (Some college or technical school), 6=College 4 years or more (College graduate)\n",
        "* Income. Income scale (INCOME2 see codebook) scale 1-8. 1=less than \\$10,000, 5=less than \\$35,000, 8=\\$75,000 or more\n",
        "\n",
        "Output variable:\n",
        "* Diabetes (0=No Risk, 1=At Risk)\n",
        "\n",
        "## Requirements\n",
        "The aim of this lab is to construct a machine learning classification model that can detect whether a patient is at risk of diabetes. The model must minimize the number of false negatives.\n",
        "\n",
        "## Acknowledgements:\n",
        "This dataset was released by the CDC."
      ],
      "metadata": {
        "id": "aTH9wwpLeE5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the imbalanced_learn library\n",
        "\n",
        "!pip install -U imbalanced-learn"
      ],
      "metadata": {
        "id": "m1Xz7iB2rSL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the diabetes_data.csv file from Github\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_3/Labs/diabetes_data.csv'"
      ],
      "metadata": {
        "id": "4y-lAtmnudl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHbo76D-bMqB"
      },
      "outputs": [],
      "source": [
        "# Load the data and create the diabetes_data DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "diabetes_data = pd.read_csv('diabetes_data.csv')\n",
        "diabetes_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any observations with missing data\n",
        "\n",
        "diabetes_data = diabetes_data.dropna()"
      ],
      "metadata": {
        "id": "MUybtpvxy4EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the structure of the data\n",
        "\n",
        "diabetes_data.info()"
      ],
      "metadata": {
        "id": "lHvHu0yYeoIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the statistics for the DataFrame\n",
        "\n",
        "diabetes_data.describe()"
      ],
      "metadata": {
        "id": "FvG1_4Yoeu-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the class ('Diabetes') and calculate the amount of imbalance\n",
        "\n",
        "has_diabetes = diabetes_data['Diabetes']\n",
        "values = has_diabetes.value_counts()\n",
        "positive = values[1]\n",
        "negative = values[0]\n",
        "print(f'Positive labels: {positive}\\nNegative labels: {negative}\\nRatio: {round(negative/positive)}:1')"
      ],
      "metadata": {
        "id": "aMmwUKbNe23M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the class from the DataFrame\n",
        "\n",
        "diabetes_data = diabetes_data.drop(['Diabetes'], axis=1)"
      ],
      "metadata": {
        "id": "in4eVvsBjH94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 5))\n",
        "column_names = diabetes_data.columns\n",
        "diabetes_data = pd.DataFrame(scaler.fit_transform(diabetes_data), columns=column_names)\n",
        "diabetes_data"
      ],
      "metadata": {
        "id": "ZZ5t0TejfODa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(diabetes_data, has_diabetes, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "c-hAqQgYn5Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create and fit an initial model using a Random Forest"
      ],
      "metadata": {
        "id": "RuAlLTDLpHK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Create the model\n",
        "forest_model = RandomForestClassifier(n_estimators=100).fit(features_train, predictions_train)\n",
        "\n",
        "# Examine the confusion matrix\n",
        "test_results = forest_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "arDt6qAooaTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these results indicate?**\n",
        "\n",
        "The model has a significant bias towards making negative predictions. The number of false negatives is high. The model is missing a lot of patients who might have diabetes."
      ],
      "metadata": {
        "id": "LuMY_sQdzku-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = forest_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "BAS9IPae0sTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the accuracy of the model \n",
        "\n",
        "print(f'Accuracy score: {forest_model.score(features_test, predictions_test)}')"
      ],
      "metadata": {
        "id": "kRticZ4eomZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does the calibration curve imply for this model?**\n",
        "\n",
        "The calbration curve shows that the model is overpredicting negative classes. The curve is below the diagonal for probabilities up to 0.9. However, it is close to the diagonal resulting in the high accuracy score. But accuracy is not the best metric for this scenario. To reduce the false negative rate you need the recall to be high rather than overall accuracy."
      ],
      "metadata": {
        "id": "V5ZqtWIH0Mp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions and calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "test_results = forest_model.predict(features_test)\n",
        "\n",
        "forest_model_gscore = geometric_mean_score(predictions_test, test_results)\n",
        "print(f'G-Mean: {forest_model_gscore}') "
      ],
      "metadata": {
        "id": "SgfOBCKxo5q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "j2JuX3kXo9zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = forest_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "forest_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "print(f'Brier score: {forest_model_bscore}')"
      ],
      "metadata": {
        "id": "tUjFilOTpCjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these metrics indicate?**\n",
        "\n",
        "The geometric mean indicates that the combined precision and recall are poor. The F0.5 and F2 scores show that precision is better that recall. Ideally for this scenario you want recall to be high, even if precision is reduced. Taken by itself, the Brier score doesn't really convey much information but it will become useful for comparison with other models later in the lab."
      ],
      "metadata": {
        "id": "zcvaDmiH7Gnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Try sampling to balance the class labels"
      ],
      "metadata": {
        "id": "0QEOAljRrA0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit a BalancedRandomForestClassifier estimator\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "ensemble_model = BalancedRandomForestClassifier(n_estimators=100) \n",
        "_ = ensemble_model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "8UMWaicrrWv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "test_results = ensemble_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "fjaHv95ermW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does the false positive and false negative rate of this model compare to the previous one?**\n",
        "\n",
        "The number of false positives has increased very significantly and number of false negatives has decreased although the false negative rate is still too high. The number of true positives is much higher than previously."
      ],
      "metadata": {
        "id": "3djC6RVG8_jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = ensemble_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean')\n",
        "plt.ylabel('Proportion') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sWUkd8Yf0c48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does this curve show?**\n",
        "\n",
        "The model has a bias towards the negative class label."
      ],
      "metadata": {
        "id": "_1LwLdh-9yxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the accuracy of the model \n",
        "\n",
        "print(f'Accuracy score: {ensemble_model.score(features_test, predictions_test)}')"
      ],
      "metadata": {
        "id": "bKnAjLtn086n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "ensemble_model_gscore = geometric_mean_score(predictions_test, test_results)\n",
        "print(f'G-Mean: {ensemble_model_gscore}') "
      ],
      "metadata": {
        "id": "GqakdVzT1Fhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "U2ZsQ-Km1QL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = ensemble_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "ensemble_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "print(f'Brier score: {ensemble_model_bscore}')"
      ],
      "metadata": {
        "id": "BTX6aLW11WKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the skill level of this model to the Random Forest model\n",
        "\n",
        "skill = 1-(ensemble_model_bscore/forest_model_bscore)\n",
        "print(f'Brier Skill score: {skill}')"
      ],
      "metadata": {
        "id": "7gs2c1s65xZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these metrics tell you?**\n",
        "\n",
        "The G-Mean and F2 scores are both better, indicating that the false negative rate has decreased. The F0.5 score has dropped slightly meaning that the false positive rate has gone up. The Brier Skill score implies that overall this model is not as good as the previous one, but this is mainly due to the large increase in false positives so it may not be critical for this model."
      ],
      "metadata": {
        "id": "2CO3l8fY-CNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare sampling to bagging"
      ],
      "metadata": {
        "id": "RLThTHGMpRXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the Random Forest classifier created earlier \n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "\n",
        "bag_model = BalancedBaggingClassifier(estimator=forest_model)\n",
        "_ = bag_model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "Tn20dEGf2i10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "test_results = bag_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "gapEFiFC38Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does this confusion matrix show?**\n",
        "\n",
        "The false negative rate has increased and the false positive rate has dropped a little. The model has moved in the wrong direction!"
      ],
      "metadata": {
        "id": "pAJdSsjOAg2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = bag_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "nGVzlk0SpQRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the accuracy of the model \n",
        "\n",
        "print(f'Accuracy score: {bag_model.score(features_test, predictions_test)}')"
      ],
      "metadata": {
        "id": "Few3uqh_pYgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "bag_model_gscore = geometric_mean_score(predictions_test, test_results)\n",
        "print(f'G-Mean: {bag_model_gscore}') "
      ],
      "metadata": {
        "id": "iC7MLL73phAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "nGbEbgS_qa1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = bag_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "bag_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "print(f'Brier score: {bag_model_bscore}')"
      ],
      "metadata": {
        "id": "9e0wHfjxqmSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the skill level of this model to the Random Forest model\n",
        "\n",
        "skill = 1-(bag_model_bscore/forest_model_bscore)\n",
        "print(f'Brier Skill score: {skill}')"
      ],
      "metadata": {
        "id": "REbfnjgLqpZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these metrics show?**\n",
        "\n",
        "The F0.5 score has increased because the number of false positives has dropped leading to slightly better precision. Recall has diminished. The Brier Skill scores imply that this model is better than the sampling model - the skill is less negative when compared to the random forest model. However, bear in mind that the false negative rate has increased, so although the combination of precision and recall have improved, recall itself has dropped."
      ],
      "metadata": {
        "id": "18Ws04H8BHpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try sampling with a different classifier - the Random Undersampler with AdaBoost (RUSBoostClasifier)"
      ],
      "metadata": {
        "id": "sQMO2fH96T_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Again, reuse the Random Forest classifier created earlier\n",
        "from imblearn.ensemble import RUSBoostClassifier\n",
        "\n",
        "rus_model = RUSBoostClassifier(estimator=forest_model)\n",
        "_ = rus_model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "h4IpEmLJ6cNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "test_results = rus_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "wZXcvdcV7USm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = rus_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "EJCpPtyl80g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What can you tell about this model?**\n",
        "\n",
        "The number of false positives have dropped as have the number of true positives. Observations which were previsously classified as positive are now being wrongly classified as negative."
      ],
      "metadata": {
        "id": "V9VyD_SUHKOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the accuracy of the model \n",
        "\n",
        "print(f'Accuracy score: {rus_model.score(features_test, predictions_test)}')"
      ],
      "metadata": {
        "id": "KkdWkp849K4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "rus_model_gscore = geometric_mean_score(predictions_test, test_results)\n",
        "print(f'G-Mean: {rus_model_gscore}') "
      ],
      "metadata": {
        "id": "b8R1g46W9P-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "wM0Bxxvm_JHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = rus_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "rus_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "print(f'Brier score: {rus_model_bscore}')"
      ],
      "metadata": {
        "id": "tzbVFRdZ_Obh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the skill level of this model to the previous models\n",
        "\n",
        "skill = 1-(rus_model_bscore/forest_model_bscore)\n",
        "print(f'Brier Skill score: {skill}')"
      ],
      "metadata": {
        "id": "Jo04gM6R_UCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these metrics show?**\n",
        "\n",
        "The F0.5 and F2 scores have both dropped. The G-Mean is also lower than the previous model. However the Brier skill score shows an improvment although you should read this score as being *less bad* rather than *good* - it is still negative."
      ],
      "metadata": {
        "id": "gMSyEtsjHxi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune the threshold for the BalancedRandomForestClassifier\n",
        "\n",
        "The BalancedRandomForestClassifier model had the lowest false negative rate of the models seen so far."
      ],
      "metadata": {
        "id": "ChgZsX8x_3wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Find the FPR, TPR, and thresholds\n",
        "probs = ensemble_model.predict_proba(features_test)\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, probs[:,1])"
      ],
      "metadata": {
        "id": "UyxPKnU8AQm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Youden's J Statistic\n",
        "J = tpr - fpr\n",
        "\n",
        "# Find the threshold at this point\n",
        "idx = np.argmax(J)\n",
        "optimal_threshold = thresholds[idx]"
      ],
      "metadata": {
        "id": "elWeiSULBF6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the results and highlight the threshold\n",
        "plt.plot(fpr, tpr, c='blue')\n",
        "plt.scatter(fpr[idx], tpr[idx], c='red', s=200)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "print(f'Optimal Threshold is {optimal_threshold}')"
      ],
      "metadata": {
        "id": "Cltp-DZJBJiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the predicted values to 1 for all predictions with a threshold >= the optimal threshold\n",
        "adjusted_predictions_test = (probs[:,1] >= optimal_threshold).astype('int')\n",
        "\n",
        "print(f'Number of test predictions affected: {np.sum(adjusted_predictions_test != predictions_test)}')"
      ],
      "metadata": {
        "id": "BX_1LeB6DlNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Precision, Recall, F1 Score, AUC, and Accuracy for the model when using the adjusted threshold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "test_results = ensemble_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(adjusted_predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(adjusted_predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(adjusted_predictions_test, test_results)}\\n')"
      ],
      "metadata": {
        "id": "n0ItHTtpAmtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the ROC curve\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "_ = metrics.RocCurveDisplay.from_predictions(adjusted_predictions_test, test_results)"
      ],
      "metadata": {
        "id": "eaq2yfd9OZm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How has this adjustment changed the false negative rate of the model?**\n",
        "\n",
        "The false negative rate has shrunk close to, if not actually, zero. In addition, the false positive rate is now also miniscule."
      ],
      "metadata": {
        "id": "zoqIK7EeKYNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve for the predictions made using the adjusted probability threshold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = ensemble_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(adjusted_predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "cHRftnrIBugi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does this curve show?**\n",
        "\n",
        "This is a classic Sigmoid Curve. The vast majority of observations with the negative class label have a probability well below the diagonal, while those with the posistive class label are above the line. The stats indicate that precision and recall are both high (99%+)"
      ],
      "metadata": {
        "id": "UUdKtG2IKvqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "ensemble_model_gscore = geometric_mean_score(adjusted_predictions_test, test_results)\n",
        "print(f'G-Mean: {ensemble_model_gscore}') "
      ],
      "metadata": {
        "id": "V_thF2_SC5sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(adjusted_predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "wTt5MAIcE9CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = ensemble_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "ensemble_model_bscore = brier_score_loss(adjusted_predictions_test, probs)\n",
        "print(f'Brier score: {ensemble_model_bscore}')"
      ],
      "metadata": {
        "id": "f68QVURdFJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the skill level of this model to the Random Forest model\n",
        "\n",
        "skill = 1-(ensemble_model_bscore/forest_model_bscore)\n",
        "print(f'Brier Skill score: {skill}')"
      ],
      "metadata": {
        "id": "s4A_26oZFk5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these metrics show?**\n",
        "\n",
        "The G-Mean and F scores are all high indicating excellent precision, recall, and accuracy. The Brier Skill score indicates that this is a better model than the original random forest.\n",
        "\n",
        "While this is an excellent result, you must be cautious and perform further testing to ensure that the model has been overfitted to the data."
      ],
      "metadata": {
        "id": "mut7H9PSNpy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try the same strategy with a different algorithm\n",
        "\n",
        "Bagging with Logistic Regression. This is just for comparison."
      ],
      "metadata": {
        "id": "paTIwnsKFww-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit a Logistic Regression classifier with the Newton CG solver (lbfgs tends not to converge with this dataset)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lg_model = BalancedBaggingClassifier(estimator=LogisticRegression(solver='newton-cg'))\n",
        "_ = lg_model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "iNx1_ydzFv5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "test_results = lg_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "mqYpgEFkG91d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = lg_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "nZliORQgHIHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Find the FPR, TPR, and thresholds for this model\n",
        "probs = lg_model.predict_proba(features_test)\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, probs[:,1])"
      ],
      "metadata": {
        "id": "-H2ZlH2gHQ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Youden's J Statistic\n",
        "J = tpr - fpr\n",
        "\n",
        "# Find the threshold at this point\n",
        "idx = np.argmax(J)\n",
        "optimal_threshold = thresholds[idx]"
      ],
      "metadata": {
        "id": "aNXWNwh_Hez7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the results and highlight the threshold\n",
        "plt.plot(fpr, tpr, c='blue')\n",
        "plt.scatter(fpr[idx], tpr[idx], c='red', s=200)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()\n",
        "\n",
        "print(f'Optimal Threshold is {optimal_threshold}')"
      ],
      "metadata": {
        "id": "Yv3zwPC6HlYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the predicted values to 1 for all predictions with a threshold >= the optimal threshold\n",
        "adjusted_predictions_test = (probs[:,1] >= optimal_threshold).astype('int')\n",
        "\n",
        "print(f'Number of test predictions affected: {np.sum(adjusted_predictions_test != predictions_test)}')"
      ],
      "metadata": {
        "id": "ZLWS-Z-YHtSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Precision, Recall, F1 Score, AUC, and Accuracy for the model when using the adjusted threshold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "test_results = lg_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(adjusted_predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(adjusted_predictions_test, test_results, average=\"macro\", zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(adjusted_predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(adjusted_predictions_test, test_results)}\\n')"
      ],
      "metadata": {
        "id": "Z7wEmqBiHzvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these results show?**\n",
        "\n",
        "The threshold was lowered rather than raised, so more observations will be classified as negative resulting in a false negative rate that isn't as good as the previous model."
      ],
      "metadata": {
        "id": "DyhHf-mGH9cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine the original Random Forest and Logistic Regression models with a Voting Classifier\n",
        "\n",
        "This is for comparison with the other models. This model aims to reduce any variance that might be caused by overfitting."
      ],
      "metadata": {
        "id": "1tQJghf3qwtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create an array containing the forest_model and lg_model estimator\n",
        "estimators = [('RF', forest_model), ('LG', lg_model)]\n",
        "\n",
        "# Create and fit a voting classifier with soft voting using the array of estimators\n",
        "vote_soft_model = VotingClassifier(estimators=estimators, voting='soft')\n",
        "_ = vote_soft_model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "71hDmmmrrSxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "test_results = vote_soft_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=[\"Negative\", \"Positive\"])"
      ],
      "metadata": {
        "id": "SHO59T2HsTtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the calibration curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve \n",
        "\n",
        "probs = vote_soft_model.predict_proba(features_test)[:,1] \n",
        "p, m = calibration_curve(predictions_test, probs, n_bins=20) \n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--') \n",
        "plt.plot(m, p, marker='.', c='red') \n",
        "plt.xlabel('Mean') \n",
        "plt.ylabel('Proportion') \n",
        "plt.show() "
      ],
      "metadata": {
        "id": "MJqWTbehsnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the G-Mean \n",
        "\n",
        "from imblearn.metrics import geometric_mean_score \n",
        "\n",
        "vote_soft_model_gscore = geometric_mean_score(predictions_test, test_results)\n",
        "print(f'G-Mean: {vote_soft_model_gscore}') "
      ],
      "metadata": {
        "id": "AZ1rK249szum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the F0.5, F1, and F2 scores\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "for beta in (0.5, 1, 2):\n",
        "  print(f'F{beta} score: {fbeta_score(predictions_test, test_results, beta=beta)}') "
      ],
      "metadata": {
        "id": "GPv-q5jCtLRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Brier score \n",
        "\n",
        "from sklearn.metrics import brier_score_loss \n",
        "\n",
        "probs = vote_soft_model.predict_proba(features_test) \n",
        "probs = probs[:, 1] # Take the probabilities for the positive class label \n",
        "\n",
        "vote_soft_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "print(f'Brier score: {vote_soft_model_bscore}')"
      ],
      "metadata": {
        "id": "Q3v8lCoGtYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the skill level of this model to the original Random Forest model\n",
        "skill = 1-(vote_soft_model_bscore/forest_model_bscore)\n",
        "print(f'Brier Skill score compared to Random Forest: {skill}')\n",
        "\n",
        "# Generate the Brier Score for the Logistic Regression model \n",
        "# and compare the skill level of the Voting Classifier model to the Logistic Regression model\n",
        "\n",
        "probs = lg_model.predict_proba(features_test) \n",
        "probs = probs[:, 1]\n",
        "\n",
        "lg_model_bscore = brier_score_loss(predictions_test, probs)\n",
        "skill = 1-(vote_soft_model_bscore/lg_model_bscore)\n",
        "print(f'Brier Skill score compared to Logistic Regression: {skill}')"
      ],
      "metadata": {
        "id": "fHrrV7Yftr63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does this model compare to those that used sampling?**\n",
        "\n",
        "In this example, the Logistic Regression model has dragged the false positive and false negative rates up."
      ],
      "metadata": {
        "id": "G0DWMiXZuD9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##If time allows\n",
        "\n",
        "Try creating a voting model combining classifiers for Gaussian Naive Bayes and K-Nearest Neighbors with the Random Forest model."
      ],
      "metadata": {
        "id": "vUvCejB16VfO"
      }
    }
  ]
}