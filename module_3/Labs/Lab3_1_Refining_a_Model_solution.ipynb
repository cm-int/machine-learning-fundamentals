{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIIb7CXtpakt"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_3/Labs/Lab3_1_Refining_a_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5267xDYARBQ9"
      },
      "source": [
        "# Lab 3.1: Refining a Machine Learning Model\n",
        "\n",
        "In this lab, you'll perform the following tasks:\n",
        "\n",
        "- Build a Logistic Regression model to classify the data without any modifications to the data\n",
        "- Examine the results and measure the performance, especially the precision\n",
        "-\tExplore and refine the dataset\n",
        "-\tRecreate and retest the model\n",
        "-\tRepeat until the performance is optimized \n",
        "\n",
        "You'll also compare the performance of two models constructed using different algorithms.\n",
        "\n",
        "## Scenario\n",
        "\n",
        "This dataset is related to white variants of the Portuguese \"Vinho Verde\" wine.The dataset describes the amount of various chemicals present in wine and their effect on it's quality. This is a binary dataset; the quality is either 'Poor' or 'Good'. Your task is to predict the quality of wine using the given data.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "\n",
        "Input variables (based on physicochemical tests):\\\n",
        "1 - fixed acidity\\\n",
        "2 - volatile acidity\\\n",
        "3 - citric acid\\\n",
        "4 - residual sugar\\\n",
        "5 - chlorides\\\n",
        "6 - free sulfur dioxide\\\n",
        "7 - total sulfur dioxide\\\n",
        "8 - density\\\n",
        "9 - pH\\\n",
        "10 - sulphates\\\n",
        "11 - alcohol\\\n",
        "12 - alkalinity\\\n",
        "13 - e330 level\\\n",
        "14 - effervescence index\\\n",
        "15 - consumable\\\n",
        "\\\n",
        "Output variable (based on sensory data):\\\n",
        "16 - quality (0=poor, 1=good)\n",
        "\n",
        "## Acknowledgements:\n",
        "This dataset is also available from Kaggle & UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzaW7tuRkLz"
      },
      "source": [
        "#Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNJwM98TMtKV"
      },
      "outputs": [],
      "source": [
        "# Upload the winequalitywhites.csv file from Github\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_3/Labs/winequalitywhites.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4CDWg2WR0Ur"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the data into a Pandas DataFrame named wine_data\n",
        "\n",
        "wine_data = pd.read_csv('winequalitywhites.csv')\n",
        "wine_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGFDkiMXTW_4"
      },
      "source": [
        "#Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpVU9zlsTvQD"
      },
      "outputs": [],
      "source": [
        "# Create the wine_features DataFrame with every column apart from quality\n",
        "\n",
        "wine_features = wine_data.drop(['quality'], axis=1)\n",
        "wine_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5mnB-UQaPn2"
      },
      "outputs": [],
      "source": [
        "# Create the wine_quality series containing only the quality column\n",
        "\n",
        "wine_quality = wine_data['quality']\n",
        "wine_quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_2vOG6KSnOC"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(wine_features, wine_quality, test_size=0.33, random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwuqaeQfbh1c"
      },
      "source": [
        "#Create a Logistic Regression model to classify the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUXIVHpabpwo"
      },
      "outputs": [],
      "source": [
        "# Create and fit the Logistic Regression model with the 'saga' solver and no regularization and an increased number of iterations and reduced tolerance (to allow the algorithm to converge)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G63RRC0HdSyi"
      },
      "outputs": [],
      "source": [
        "# Test the model and examine the confusion matrix\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay \n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVjWGZrofl3x"
      },
      "outputs": [],
      "source": [
        "# Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNbFU7oMfRBg"
      },
      "outputs": [],
      "source": [
        "# Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0YGi7KxSPDt"
      },
      "outputs": [],
      "source": [
        "# Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfsej96EjnGv"
      },
      "outputs": [],
      "source": [
        "# Find the threshold that maximizes precision and recall for the 'good' (1) class label\n",
        "# Display the F1 score, precision, and recall for this threshold\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "precision, recall, thresholds = precision_recall_curve(predictions_test, test_results_proba[:, 1])\n",
        "\n",
        "precision[precision == 0] = 1e-99\n",
        "recall[recall == 0] = 1e-99\n",
        "fscores = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "ix = np.argmax(fscores)\n",
        "print(f'Optimal threshold is {thresholds[ix]}\\nF1 Score is {fscores[ix]}\\nPrecision is {precision[ix]}\\nRecall is {recall[ix]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NRB4f1v8l4"
      },
      "source": [
        "**What do you conclude from these statistics?**\n",
        "\n",
        "The precision indicates that the model has a large false positive rate. Many wines classified as having *good* quality are actually *poor*.\n",
        "\n",
        "The recall shows that the model has a much smaller false negative rate. A few wines that are classified as *poor* should actually be *good*.\n",
        "\n",
        "The high recall but low precision results in a misleadingly high F1 score.\n",
        "\n",
        "The AUC indicates that the model is performing no better than random guesswork.\n",
        "\n",
        "These statistics show that you should never use one measurement in isolation to judge the performance of a model.\n",
        "\n",
        "The model *may* appear work better with a probability threshold of 0.484 for the class labels; predictions with a probability less than this value should be a 0, and those at or above this value should be a 1. However, the precision indicates that reducing the threshold is likely to increase the already substantial number of false positives (the precision will dropp) and only reduce the number of false negatives (the recall will improve); it makes a biased model even more biased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUhS_1xUQXUM"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJXGRJqKpBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the Gini Coefficient for the model\n",
        "# Gini Coefficient=2×(AUC−1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC17z5zuQomx"
      },
      "source": [
        "**What does this coefficient signify?**\n",
        "\n",
        "A Gini Coefficient of 0.01 indicates the model has very poor performance. Ideally, you should aim for a Gini Coefficient greater than 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR5R3of2QiRC"
      },
      "outputs": [],
      "source": [
        "# Calculate Cohen's Kappa for the model\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2UGbBtRFZL"
      },
      "source": [
        "**What does this value mean?**\n",
        "\n",
        "The Cohen's Kappa value lies between 0.01 and 0.2. This indicates that there is very slight agreement between the model and the real observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl8t_RCNxKm3"
      },
      "outputs": [],
      "source": [
        "# Calculate the Hamming Loss for the model\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lpiB0VqI5Wq"
      },
      "source": [
        "**What proportion of the predictions are incorrect?**\n",
        "\n",
        "The Hamming Loss indicates that 33.5% of the predictions are incorrect. This model is a poor fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tF9Vxm-Ul54"
      },
      "outputs": [],
      "source": [
        "# Calculate the Matthews Correlation Coefficient for the model\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzYaTlOiV-FV"
      },
      "source": [
        "**How strong is the relationship between the predicted and observed class labels?**\n",
        "\n",
        "The relationship is between 0 and 0.19, which means there is a negligable relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_8-hXyHR76u"
      },
      "outputs": [],
      "source": [
        "# Plot the cumulative gains chart for the model\n",
        "!pip install Scikit-plot\n",
        "\n",
        "import scikitplot as skplt\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks5SrAUswk6R"
      },
      "source": [
        "**Overall, do your findings confirm your earlier conclusions about the precision and recall of the model?**\n",
        "\n",
        "All of the metrics confirm that the model is currently a very poor fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H94-dXGQICT"
      },
      "source": [
        "# Refine the model - scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTbicWjZikqy"
      },
      "outputs": [],
      "source": [
        "# Apply a MinMaxScaler to the wine_features dataframe \n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "column_names = wine_features.columns\n",
        "scaled_wine_features = pd.DataFrame(scaler.fit_transform(wine_features), columns=column_names)\n",
        "\n",
        "scaled_wine_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XWkHX0QACN1"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with scaled features:\n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(scaled_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7IUUc4Gf_A3"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2cD3EazgbWr"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVXXd2PWkHnm"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "There has been a notable improvement in all metrics. The false positive and false negative rates have both decreased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONHjQWWuxuC"
      },
      "source": [
        "# Refine the model - remove constant and quasi-constant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx6Ft_Dnuwsg"
      },
      "outputs": [],
      "source": [
        "# Look for features with little variance in the scaled dataframe\n",
        "\n",
        "print(scaled_wine_features.var())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuMNbrdQhYQA"
      },
      "source": [
        "**Which features have a notably small variance?**\n",
        "\n",
        "The *consumable* feature has zero variance, so has the same value in every observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDdrrNsYhxFp"
      },
      "outputs": [],
      "source": [
        "# Verify that 'consumable' has only one value - display all the unique values in this feature\n",
        "\n",
        "print(np.unique(scaled_wine_features['consumable']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vvKSChCMfDB"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model without this feature:\n",
        "# - Drop the feature from the scaled_wine_features dataframe  \n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "no_constants_wine_features = scaled_wine_features.drop(['consumable'], axis=1)\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_constants_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPE8xeaKMl7P"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dPqfqzZMptw"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt_P1ZdVm2h6"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "There has been no effect on predictive power. The constant column was probably not an important part of the model, but it makes sense to remove it to save resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-oeloM1MsyT"
      },
      "source": [
        "# Refine the model - find and remove correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlQszPvZvsHB"
      },
      "outputs": [],
      "source": [
        "# Find correlated features in the scaled and reduced dataset\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "correlation_matrix = no_constants_wine_features.corr(method='kendall')\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(correlation_matrix, annot=True, linecolor='black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEoldubAqN3c"
      },
      "source": [
        "**Which features show a strong correlation?**\n",
        "\n",
        "The 'e330.level' and 'citric acid' features have a positive correlation coefficient of 1, meaning that they convey the same information.\n",
        "\n",
        "The 'pH' and 'alkalinity' columns have a negative correlation coefficient of -1. Alkalinity is the exact converse of pH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR8FVhvyM3lH"
      },
      "outputs": [],
      "source": [
        "# Remove the e330.level and alkalinity features and rebuild the model\n",
        "# - Drop the features from the scaled_wine_features dataframe  \n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "no_correlation_wine_features = no_constants_wine_features.drop(['e330.level', 'alkalinity'], axis=1)\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_correlation_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3L_3hNcM5mU"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm43T4V8NEUy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tly86b1yNG7G"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "No, but you shouldn't necessarily expect it to have done. Like removing constant and quasi-constant features, the purpose of removing correlated features is to minimize the resources required to build and use the model. The important point is that the model shouldn't be worse as a result. In this case, the metrics are the same as the previous model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZdlcWvNMR2"
      },
      "source": [
        "# Refine the model - remove noise using univariate feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-YR2pIZz_--"
      },
      "outputs": [],
      "source": [
        "# Perform SHAP analysis to find the features that have the most impact on predictions\n",
        "\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(wine_model.predict, features_test) \n",
        "values = explainer(features_train)\n",
        "\n",
        "shap.summary_plot(shap_values=values, features=features_train, plot_type=\"bar\")\n",
        "shap.summary_plot(shap_values=values, features=features_train, plot_type=\"violin\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DzG7cy_0Iha"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with only the top five features\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "reduced_wine_features = no_correlation_wine_features[['density', 'residual.sugar', 'alcohol', 'volatile.acidity', 'pH']]\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(reduced_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82EoEXLt0mMy"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc0RvnM50aax"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNxK6PeyJ46B"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "There is a marginal improvement in the number of true positives and false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xicsOkEqL6h"
      },
      "source": [
        "# Refine the model - find the combination of features that give the lowest false positive rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YICjaTbeqKqd"
      },
      "outputs": [],
      "source": [
        "# Use selectFpr() function to find the best combination of features that minimize the FPR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFpr\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_correlation_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "features_selector = SelectFpr(score_func=chi2)\n",
        "_ = features_selector.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TV9ELbJrpVe"
      },
      "outputs": [],
      "source": [
        "# Print the feature names and scores\n",
        "\n",
        "feature_names = features_selector.get_feature_names_out()\n",
        "\n",
        "for i in range(len(feature_names)):\n",
        "\tprint(f'Feature {feature_names[i]}: {features_selector.scores_[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGhp7FHCsKhl"
      },
      "source": [
        "**How does this compare to the features found by using SHAP analysis?**\n",
        "\n",
        "The list found by using forward selection was 'density', 'residual.sugar', 'alcohol', 'volatile.acidity', and 'pH'. These results suggest that selecting the features 'volatile.acidity', 'chlorides', 'density', and 'alcohol' will give the lowest FPR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqos-Dfnt4H4"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with these features\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "reduced_wine_features = no_correlation_wine_features[['volatile.acidity', 'chlorides', 'density', 'alcohol']]\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(reduced_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCa39Bf4uO_l"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_BupWLOx7KM"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NMDOHPIzJ8U"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "There is a very small decrease in the FPR but also a decrease in the TPR and an increase in the FNR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJIIyYltLHeM"
      },
      "source": [
        "# Refine the model - remove noise using multivariate feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoMs4sBzK9Ew"
      },
      "outputs": [],
      "source": [
        "# Use forward selection to find the best combination of features\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "logistic_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3) \n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_correlation_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "sfs_forward = SequentialFeatureSelector(logistic_model, n_features_to_select=5, direction=\"forward\")\n",
        "_ = sfs_forward.fit(features_train, predictions_train) \n",
        "\n",
        "print(f'Features selected by forward sequential selection: {sfs_forward.get_feature_names_out()}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYnq2R3fo7dC"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with only the top five features\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "reduced_wine_features = no_correlation_wine_features[sfs_forward.get_feature_names_out()]\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(reduced_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC9WbjdVpUmv"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "test_results = wine_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(wine_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grLrpKIupzcf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYMDFxkqDoW"
      },
      "source": [
        "**Has the model improved?**\n",
        "\n",
        "Multivariate forward selection has produced a better model overall than univariate feature selection. The Matthews Correlation Coefficient now indicates a strong relationship between the observed values and predictions made by the model, although the Gini Coefficient is still relatively low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O52XDvEiSYxY"
      },
      "source": [
        "# Investigate the impact of regularization on the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHf_bjmrTLcg"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model before regularization\n",
        "\n",
        "from sklearn.model_selection import learning_curve, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_constants_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)\n",
        "\n",
        "# Compute the data for the learning curve using 10-fold cross validation of the model\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=wine_model, X=features_train, y=predictions_train, train_sizes=np.linspace(0.1, 1.0, 19), cv=10, scoring='precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXomnA-Z4x2p"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot((0,3000), (0.75,0.75), c='Grey', alpha=0.5)\n",
        "plt.plot((0,3000), (0.80,0.80), c='Grey', alpha=0.5)\n",
        "plt.plot(train_sizes, np.mean(train_scores,axis=1), label='Train (no penalty)')\n",
        "plt.plot(train_sizes, np.mean(test_scores,axis=1), label='Test (no penalty)')\n",
        "plt.xlabel('Dataset Size', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylabel('Precision', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylim(bottom=0.7, top=0.85)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "print(f'Best test score precision: {np.max(np.mean(test_scores,axis=1))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCqDf9XyTYpF"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with L1 regularization\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='l1', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)\n",
        "\n",
        "# Compute the data for the learning curve using 10-fold cross validation of the model\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=wine_model, X=features_train, y=predictions_train, train_sizes=np.linspace(0.1, 1.0, 19), cv=10, scoring='precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADlgbve46a9k"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot((0,3000), (0.75,0.75), c='Grey', alpha=0.5)\n",
        "plt.plot((0,3000), (0.80,0.80), c='Grey', alpha=0.5)\n",
        "plt.plot(train_sizes, np.mean(train_scores,axis=1), label='Train (L1)')\n",
        "plt.plot(train_sizes, np.mean(test_scores,axis=1), label='Test (L1)')\n",
        "plt.xlabel('Dataset Size', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylabel('Precision', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylim(bottom=0.7, top=0.85)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "print(f'Best test score precision: {np.max(np.mean(test_scores,axis=1))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PShX5pVMTdGn"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with L2 regularization\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='l2', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)\n",
        "\n",
        "# Compute the data for the learning curve using 10-fold cross validation of the model\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=wine_model, X=features_train, y=predictions_train, train_sizes=np.linspace(0.1, 1.0, 19), cv=10, scoring='precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhfmQhKxAMuP"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot((0,3000), (0.75,0.75), c='Grey', alpha=0.5)\n",
        "plt.plot((0,3000), (0.80,0.80), c='Grey', alpha=0.5)\n",
        "plt.plot(train_sizes, np.mean(train_scores,axis=1), label='Train (L2)')\n",
        "plt.plot(train_sizes, np.mean(test_scores,axis=1), label='Test (L2)')\n",
        "plt.xlabel('Dataset Size', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylabel('Precision', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylim(bottom=0.7, top=0.85)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "print(f'Best test score precision: {np.max(np.mean(test_scores,axis=1))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r907O-pTgUG"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with Elastic Net regularization\n",
        "\n",
        "wine_model = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)\n",
        "\n",
        "# Compute the data for the learning curve using 10-fold cross validation of the model\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=wine_model, X=features_train, y=predictions_train, train_sizes=np.linspace(0.1, 1.0, 19), cv=10, scoring='precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXK4cInsAhUh"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot((0,3000), (0.75,0.75), c='Grey', alpha=0.5)\n",
        "plt.plot((0,3000), (0.80,0.80), c='Grey', alpha=0.5)\n",
        "plt.plot(train_sizes, np.mean(train_scores,axis=1), label='Train (Elastic Net)')\n",
        "plt.plot(train_sizes, np.mean(test_scores,axis=1), label='Test (Elastic Net)')\n",
        "plt.xlabel('Dataset Size', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylabel('Precision', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylim(bottom=0.7, top=0.85)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "print(f'Best test score precision: {np.max(np.mean(test_scores,axis=1))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8BQ89gbTmU6"
      },
      "source": [
        "**What do you conclude about applying the different forms of regularization to this model?**\n",
        "\n",
        "L1 regularization and L2 regularization both have a small detrimental effect. The model is not being overfitted, so regularization is probably unnecessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_5M887WSzvn"
      },
      "source": [
        "# Compare the Logistic Regression model to a Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zucKLgRZT8X1"
      },
      "outputs": [],
      "source": [
        "# Create a random forest model over the same data\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_model = RandomForestClassifier()\n",
        "_ = forest_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on1s0B4VG0AF"
      },
      "outputs": [],
      "source": [
        "# Test the random forest model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "rf_test_results = forest_model.predict(features_test)\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, rf_test_results)\n",
        "\n",
        "print(f'Precision: {precision_score(predictions_test, rf_test_results, zero_division=0)}\\n')\n",
        "print(f'Recall: {recall_score(predictions_test, test_results, zero_division=0)}\\n')\n",
        "print(f'F1 Score: {f1_score(predictions_test, rf_test_results, zero_division=0)}\\n')\n",
        "print(f'AUC: {roc_auc_score(predictions_test, rf_test_results)}\\n')\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, test_results)}\\n')\n",
        "\n",
        "display = RocCurveDisplay.from_estimator(forest_model, features_test, predictions_test)\n",
        "display = RocCurveDisplay.from_predictions(predictions_test, rf_test_results)\n",
        "\n",
        "_ = PrecisionRecallDisplay.from_estimator(forest_model, features_test, predictions_test)\n",
        "display = PrecisionRecallDisplay.from_predictions(predictions_test, rf_test_results)\n",
        "_ = display.ax_.set_ylim(bottom=0, top=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1lVxSqAHSs2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n",
        "auc = roc_auc_score(predictions_test, rf_test_results)\n",
        "gini_coeff = (2 * auc) - 1\n",
        "print(f'Gini Coefficient is: {gini_coeff}')\n",
        "\n",
        "kappa_score = cohen_kappa_score(predictions_test, rf_test_results)\n",
        "print(f\"Cohen's Kappa is: {kappa_score}\")\n",
        "\n",
        "hamming_score = hamming_loss(predictions_test, rf_test_results)\n",
        "print(f'Hamming Loss is: {hamming_score}')\n",
        "\n",
        "mcc = matthews_corrcoef(predictions_test, rf_test_results)\n",
        "print(f'Matthews Correlation Coefficient is: {mcc}')\n",
        "\n",
        "rf_test_results_proba = forest_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, rf_test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9KwfPsiHuCX"
      },
      "source": [
        "**How does this model compare to the Logistic Regression model?**\n",
        "\n",
        "The random forest model has significantly better recall and precision than the logistic regression model. Overall its performance is superior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvJ33oYLUUOe"
      },
      "outputs": [],
      "source": [
        "# Perform McNemar's test to compare the error rates of the models\n",
        "\n",
        "!pip install Mlxtend\n",
        "\n",
        "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
        "\n",
        "table = mcnemar_table(y_target=predictions_test, y_model1=test_results, y_model2=rf_test_results)\n",
        "\n",
        "chi2, p = mcnemar(ary=table, corrected=True)\n",
        "print(f'\\nContingency table\\n{table}')\n",
        "print(f'\\nchi-squared statistic: {chi2}, p-value: {p}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaosZ1wnLJX_"
      },
      "source": [
        "**What does this test indicate?**\n",
        "\n",
        "The p-value is very small 0.05. The difference in error rates between the two models is statistically significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZAZSeRLGBFm"
      },
      "outputs": [],
      "source": [
        "# Perform 5x2 cross-validation test to compare the models\n",
        "\n",
        "from mlxtend.evaluate import paired_ttest_5x2cv\n",
        "\n",
        "t, p = paired_ttest_5x2cv(estimator1=wine_model, estimator2=forest_model, X=features_train, y=predictions_train)\n",
        "\n",
        "print(f't-statistic: {t}')\n",
        "print(f'p-value: {p}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GoAO65XISex"
      },
      "source": [
        "**Is there a significance in the difference of the accuracy of the two models?**\n",
        "\n",
        "The p-value is very low and is below the accepted threshold of 5% for statistical significance. This result indicates that although there is a statistically significant difference in the accuracy of the two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk9d6hXYUBBU"
      },
      "outputs": [],
      "source": [
        "# Compare the DET curves for the two models\n",
        "\n",
        "from sklearn.metrics import DetCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax_det = plt.subplots(1, 1, figsize=(10, 10))\n",
        "_ = DetCurveDisplay.from_estimator(wine_model, features_test, predictions_test, ax=ax_det, name='Logistic Regression Model')\n",
        "\n",
        "_ = DetCurveDisplay.from_estimator(forest_model, features_test, predictions_test, ax=ax_det, name='Random Forest Model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-meHfRVUNA7"
      },
      "source": [
        "**How does the Logistic Regression model compare to the Random Forest model**\n",
        "\n",
        "The DET curve shows that the Random Forest model generally has a lower error rate that the Logistic Regression model. Remember that the axes on this graph have a non-linear scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4vz7PtFUgC2"
      },
      "source": [
        "#Conclusions\n",
        "\n",
        "It is important to understand how to measure the effects of tuning a model in different ways, and how to compare the performance of two models.\n",
        "\n",
        "Scaling the features can has a notable effect on a linear model, although the results will likely be less dramatic on a tree-based model.\n",
        "\n",
        "This exercise also highlights that algorithm selection is an important part of building a machine learning classification model. The random forest model worked much better than the logistic regression model, even without performing any tuning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
