{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIIb7CXtpakt"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_3/Labs/Lab3_1_Refining_a_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5267xDYARBQ9"
      },
      "source": [
        "# Lab 3.1: Refining a Machine Learning Model\n",
        "\n",
        "In this lab, you'll perform the following tasks:\n",
        "\n",
        "- Build a Logistic Regression model to classify the data without any modifications to the data\n",
        "- Examine the results and measure the performance, especially the precision\n",
        "-\tExplore and refine the dataset\n",
        "-\tRecreate and retest the model\n",
        "-\tRepeat until the performance is optimized \n",
        "\n",
        "You'll also compare the performance of two models constructed using different algorithms.\n",
        "\n",
        "## Scenario\n",
        "\n",
        "This dataset is related to white variants of the Portuguese \"Vinho Verde\" wine.The dataset describes the amount of various chemicals present in wine and their effect on it's quality. This is a binary dataset; the quality is either 'Poor' or 'Good'. Your task is to predict the quality of wine using the given data.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "\n",
        "Input variables (based on physicochemical tests):\\\n",
        "1 - fixed acidity\\\n",
        "2 - volatile acidity\\\n",
        "3 - citric acid\\\n",
        "4 - residual sugar\\\n",
        "5 - chlorides\\\n",
        "6 - free sulfur dioxide\\\n",
        "7 - total sulfur dioxide\\\n",
        "8 - density\\\n",
        "9 - pH\\\n",
        "10 - sulphates\\\n",
        "11 - alcohol\\\n",
        "12 - alkalinity\\\n",
        "13 - e330 level\\\n",
        "14 - effervescence index\\\n",
        "15 - consumable\\\n",
        "\\\n",
        "Output variable (based on sensory data):\\\n",
        "16 - quality (0=poor, 1=good)\n",
        "\n",
        "## Acknowledgements:\n",
        "This dataset is also available from Kaggle & UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdOienfNtLpK"
      },
      "source": [
        "The solution code for this lab is available <a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_3/Labs/Lab3_1_Refining_a_Model_solution.ipynb\" target=\"_parent\">here</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzaW7tuRkLz"
      },
      "source": [
        "#Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNJwM98TMtKV"
      },
      "outputs": [],
      "source": [
        "# Upload the winequalitywhites.csv file from Github\n",
        "# This step is complete\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_3/Labs/winequalitywhites.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4CDWg2WR0Ur"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the data into a Pandas DataFrame named wine_data\n",
        "# This step is complete\n",
        "\n",
        "wine_data = pd.read_csv('winequalitywhites.csv')\n",
        "wine_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGFDkiMXTW_4"
      },
      "source": [
        "#Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpVU9zlsTvQD"
      },
      "outputs": [],
      "source": [
        "# Create the wine_features DataFrame with every column apart from quality\n",
        "# This step is complete\n",
        "\n",
        "wine_features = wine_data.drop(['quality'], axis=1)\n",
        "wine_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5mnB-UQaPn2"
      },
      "outputs": [],
      "source": [
        "# Create the wine_quality series containing only the quality column\n",
        "# This step is complete\n",
        "\n",
        "wine_quality = wine_data['quality']\n",
        "wine_quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_2vOG6KSnOC"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test datasets\n",
        "# This step is complete\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(wine_features, wine_quality, test_size=0.33, random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwuqaeQfbh1c"
      },
      "source": [
        "#Create a Logistic Regression model to classify the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUXIVHpabpwo"
      },
      "outputs": [],
      "source": [
        "# Create and fit the Logistic Regression model named 'wine_model' with the 'saga' solver and no regularization and an increased number of iterations and reduced tolerance (to allow the algorithm to converge)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G63RRC0HdSyi"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the features_test dataset (save the results as the variable 'test_results') and examine the confusion matrix\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVjWGZrofl3x"
      },
      "outputs": [],
      "source": [
        "# Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNbFU7oMfRBg"
      },
      "outputs": [],
      "source": [
        "# Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0YGi7KxSPDt"
      },
      "outputs": [],
      "source": [
        "# Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfsej96EjnGv"
      },
      "outputs": [],
      "source": [
        "# Find the threshold that maximizes precision and recall for the 'good' (1) class label\n",
        "# Display the F1 score, precision, and recall for this threshold\n",
        "\n",
        "# This step is complete\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "precision, recall, thresholds = precision_recall_curve(predictions_test, test_results_proba[:, 1])\n",
        "\n",
        "precision[precision == 0] = 1e-99\n",
        "recall[recall == 0] = 1e-99\n",
        "fscores = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "ix = np.argmax(fscores)\n",
        "print(f'Optimal threshold is {thresholds[ix]}\\nF1 Score is {fscores[ix]}\\nPrecision is {precision[ix]}\\nRecall is {recall[ix]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NRB4f1v8l4"
      },
      "source": [
        "**What do you conclude from these statistics?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUhS_1xUQXUM"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jJXGRJqKpBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the Gini Coefficient for the model\n",
        "# Gini Coefficient=2×(AUC−1)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC17z5zuQomx"
      },
      "source": [
        "**What does this coefficient signify?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR5R3of2QiRC"
      },
      "outputs": [],
      "source": [
        "# Calculate Cohen's Kappa for the model\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2UGbBtRFZL"
      },
      "source": [
        "**What does this value mean?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl8t_RCNxKm3"
      },
      "outputs": [],
      "source": [
        "# Calculate the Hamming Loss for the model\n",
        "from sklearn.metrics import hamming_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lpiB0VqI5Wq"
      },
      "source": [
        "**What proportion of the predictions are incorrect?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tF9Vxm-Ul54"
      },
      "outputs": [],
      "source": [
        "# Calculate the Matthews Correlation Coefficient for the model\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzYaTlOiV-FV"
      },
      "source": [
        "**How strong is the relationship between the predicted and observed class labels?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_8-hXyHR76u"
      },
      "outputs": [],
      "source": [
        "# Plot the cumulative gains chart for the model\n",
        "!pip install Scikit-plot\n",
        "\n",
        "import scikitplot as skplt\n",
        "\n",
        "test_results_proba = wine_model.predict_proba(features_test)\n",
        "_ = skplt.metrics.plot_cumulative_gain(predictions_test, test_results_proba, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks5SrAUswk6R"
      },
      "source": [
        "**Overall, do your findings confirm your earlier conclusions about the precision and recall of the model?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H94-dXGQICT"
      },
      "source": [
        "# Refine the model - scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTbicWjZikqy"
      },
      "outputs": [],
      "source": [
        "# Apply a MinMaxScaler to the wine_features dataframe to create a new dataframe named 'scaled_wine_features'\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XWkHX0QACN1"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with scaled features:\n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7IUUc4Gf_A3"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2cD3EazgbWr"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVXXd2PWkHnm"
      },
      "source": [
        "**Has the model improved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONHjQWWuxuC"
      },
      "source": [
        "# Refine the model - remove constant and quasi-constant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx6Ft_Dnuwsg"
      },
      "outputs": [],
      "source": [
        "# Look for features with little variance in the scaled dataframe using the 'var()' function of the dataframe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuMNbrdQhYQA"
      },
      "source": [
        "**Which features have a notably small variance?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDdrrNsYhxFp"
      },
      "outputs": [],
      "source": [
        "# Verify that 'consumable' has only one value - display all the unique values in this feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vvKSChCMfDB"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model without this feature:\n",
        "# - Drop the feature from the scaled_wine_features dataframe to create a dataframe named 'no_constants_wine_features'\n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPE8xeaKMl7P"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dPqfqzZMptw"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt_P1ZdVm2h6"
      },
      "source": [
        "**Has the model improved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-oeloM1MsyT"
      },
      "source": [
        "# Refine the model - find and remove correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlQszPvZvsHB"
      },
      "outputs": [],
      "source": [
        "# Find correlated features in the 'no_constants_wine_features' dataset\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEoldubAqN3c"
      },
      "source": [
        "**Which features show a strong correlation?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR8FVhvyM3lH"
      },
      "outputs": [],
      "source": [
        "# Remove the e330.level and alkalinity features and rebuild the model\n",
        "# - Drop the features from the no_constants_wine_features dataframe to create the no_correlation_wine_features dataset\n",
        "# - Recreate test and training datasets\n",
        "# - Build the Logistic Regression model with the same parameters as before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3L_3hNcM5mU"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "# - Make predictions and examine the confusion matrix\n",
        "# - Calculate the precision, recall, F1-score, AUC and accuracy for the model\n",
        "# - Plot the ROC curve for the model from the estimator and from the test predictions\n",
        "# - Plot the Precision/Recall graph for the model using the estimator and from the test results\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm43T4V8NEUy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "# - Calculate the Gini Coefficient for the model\n",
        "# - Calculate Cohen's Kappa\n",
        "# - Calculate the Hamming Loss\n",
        "# - Calculate the Matthews Correlation Coefficient\n",
        "# - Plot the cumulative gains chart for the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tly86b1yNG7G"
      },
      "source": [
        "**Has the model improved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZdlcWvNMR2"
      },
      "source": [
        "# Refine the model - remove noise using univariate feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-YR2pIZz_--"
      },
      "outputs": [],
      "source": [
        "# Perform SHAP analysis to find the features that have the most impact on predictions\n",
        "# This step is complete\n",
        "\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(wine_model.predict, features_test) \n",
        "values = explainer(features_train)\n",
        "\n",
        "shap.summary_plot(shap_values=values, features=features_train, plot_type=\"bar\")\n",
        "shap.summary_plot(shap_values=values, features=features_train, plot_type=\"violin\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DzG7cy_0Iha"
      },
      "outputs": [],
      "source": [
        "# Create a dataset named 'reduced_wine_features' from the 'no_correlation_wine_features' dataset with only the top five features and rebuild the model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82EoEXLt0mMy"
      },
      "outputs": [],
      "source": [
        "# Test the model (repeat all previous tests)\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc0RvnM50aax"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model (repeat all previous tests)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNxK6PeyJ46B"
      },
      "source": [
        "**Has the model improved?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xicsOkEqL6h"
      },
      "source": [
        "# Refine the model - find the combination of features that give the lowest false positive rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YICjaTbeqKqd"
      },
      "outputs": [],
      "source": [
        "# Use selectFpr() function to find the best combination of features that minimize the FPR using the 'no_correlation_wine_features' dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFpr\n",
        "from sklearn.feature_selection import chi2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TV9ELbJrpVe"
      },
      "outputs": [],
      "source": [
        "# Print the feature names and scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGhp7FHCsKhl"
      },
      "source": [
        "**How does this compare to the features found by using SHAP analysis?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqos-Dfnt4H4"
      },
      "outputs": [],
      "source": [
        "# Rebuild and fit the model with these features. Name the new dataframe 'reduced_wine_features'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCa39Bf4uO_l"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_BupWLOx7KM"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NMDOHPIzJ8U"
      },
      "source": [
        "**Has the model improved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJIIyYltLHeM"
      },
      "source": [
        "# Refine the model - remove noise using multivariate feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoMs4sBzK9Ew"
      },
      "outputs": [],
      "source": [
        "# Use forward selection to find the best combination of features\n",
        "# This step is complete\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "logistic_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3) \n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_correlation_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "\n",
        "sfs_forward = SequentialFeatureSelector(logistic_model, n_features_to_select=5, direction=\"forward\")\n",
        "_ = sfs_forward.fit(features_train, predictions_train) \n",
        "\n",
        "print(f'Features selected by forward sequential selection: {sfs_forward.get_feature_names_out()}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYnq2R3fo7dC"
      },
      "outputs": [],
      "source": [
        "# Rebuild the model with only the top five features\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC9WbjdVpUmv"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grLrpKIupzcf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYMDFxkqDoW"
      },
      "source": [
        "**Has the model improved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O52XDvEiSYxY"
      },
      "source": [
        "# Investigate the impact of regularization on the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHf_bjmrTLcg"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model before regularization. Use the 'no_constants_wine_features' dataframe to generate the test and training datasets\n",
        "# This step is complete\n",
        "\n",
        "from sklearn.model_selection import learning_curve, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(no_constants_wine_features, wine_quality, test_size=0.33, random_state=13)\n",
        "wine_model = LogisticRegression(solver='saga', penalty='none', max_iter=2000, tol=1e-3)\n",
        "_ = wine_model.fit(features_train, predictions_train)\n",
        "\n",
        "# Compute the data for the learning curve using 10-fold cross validation of the model\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=wine_model, X=features_train, y=predictions_train, train_sizes=np.linspace(0.1, 1.0, 19), cv=10, scoring='precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXomnA-Z4x2p"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "# This step is complete\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot((0,3000), (0.75,0.75), c='Grey', alpha=0.5)\n",
        "plt.plot((0,3000), (0.80,0.80), c='Grey', alpha=0.5)\n",
        "plt.plot(train_sizes, np.mean(train_scores,axis=1), label='Train (no penalty)')\n",
        "plt.plot(train_sizes, np.mean(test_scores,axis=1), label='Test (no penalty)')\n",
        "plt.xlabel('Dataset Size', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylabel('Precision', fontdict={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size': 28})\n",
        "plt.ylim(bottom=0.7, top=0.85)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "print(f'Best test score precision: {np.max(np.mean(test_scores,axis=1))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCqDf9XyTYpF"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with L1 regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADlgbve46a9k"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PShX5pVMTdGn"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with L2 regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhfmQhKxAMuP"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r907O-pTgUG"
      },
      "outputs": [],
      "source": [
        "# Measure the learning rate of the model with Elastic Net regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXK4cInsAhUh"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8BQ89gbTmU6"
      },
      "source": [
        "**What do you conclude about applying the different forms of regularization to this model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_5M887WSzvn"
      },
      "source": [
        "# Compare the Logistic Regression model to a Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zucKLgRZT8X1"
      },
      "outputs": [],
      "source": [
        "# Create a random forest model over the same data\n",
        "# This step is complete\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_model = RandomForestClassifier()\n",
        "_ = forest_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on1s0B4VG0AF"
      },
      "outputs": [],
      "source": [
        "# Test the random forest model\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1lVxSqAHSs2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, cohen_kappa_score, hamming_loss, log_loss, matthews_corrcoef\n",
        "import scikitplot as skplt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9KwfPsiHuCX"
      },
      "source": [
        "**How does this model compare to the Logistic Regression model?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvJ33oYLUUOe"
      },
      "outputs": [],
      "source": [
        "# Perform McNemar's test to compute the marginal error rates of the models and compare these error rates\n",
        "\n",
        "!pip install Mlxtend\n",
        "\n",
        "from mlxtend.evaluate import mcnemar_table, mcnemar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaosZ1wnLJX_"
      },
      "source": [
        "**What does this test indicate?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZAZSeRLGBFm"
      },
      "outputs": [],
      "source": [
        "# Perform 5x2 cross-validation test to compare the models\n",
        "\n",
        "from mlxtend.evaluate import paired_ttest_5x2cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GoAO65XISex"
      },
      "source": [
        "**Is there a significance in the difference of the accuracy of the two models?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk9d6hXYUBBU"
      },
      "outputs": [],
      "source": [
        "# Compare the DET curves for the two models\n",
        "\n",
        "from sklearn.metrics import DetCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-meHfRVUNA7"
      },
      "source": [
        "**How does the Logistic Regression model compare to the Random Forest model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4vz7PtFUgC2"
      },
      "source": [
        "#Conclusions\n",
        "\n",
        "It is important to understand how to measure the effects of tuning a model in different ways, and how to compare the performance of two models.\n",
        "\n",
        "Scaling the features can has a notable effect on a linear model, although the results will likely be less dramatic on a tree-based model.\n",
        "\n",
        "This exercise also highlights that algorithm selection is an important part of building a machine learning classification model. The random forest model worked much better than the logistic regression model, even without performing any tuning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
