{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_2/Democode/Mod_2_Lesson_3_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9eiIWAqmFJJ"
      },
      "source": [
        "# Validating a Binary Classification Machine Learning Model\n",
        "\n",
        "In this demonstration, you’ll build and test a Gradient Boosted Tree model. You will measure the accuracy, precision, recall and AUC for the model, and then perform a grid search to optimize the hyperparameters for the model. You’ll also assess the model for its response to bias and variance in the data.\n",
        "\n",
        "This demonstration uses a modified version of the **Mushroom Classification: Safe to eat or deadly poison?** dataset originally donated to the UCI Machine Learning repository. It is available for use under the **CC0: Public Domain** licence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV5QfJpimVKH"
      },
      "source": [
        "# Upload and prepare the data\n",
        "\n",
        "This is the same dataset used by the previous demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_2/Democode/mushrooms.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJk4kSe9mmoM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mushrooms = pd.read_csv(\"mushrooms.csv\")\n",
        "mushrooms = pd.get_dummies(mushrooms)\n",
        "features = mushrooms.drop(['class_e', 'class_p'], axis=1)\n",
        "predictions = mushrooms['class_e']\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(features, predictions, test_size=0.33, random_state=13) # Random state specified to ensure repeatability if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR6r50u0m8kW"
      },
      "source": [
        "# Create a Gradient Boosted Tree model to classify the data\n",
        "\n",
        "This is the same procedure used by the previous demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xGa0y9nnCq-"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbt_model = GradientBoostingClassifier(n_estimators=11, learning_rate=3, criterion='squared_error', max_depth=10) # NOTE: These parameters have been deliberately chosen to generate a poor model\n",
        "_= gbt_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2SGZtiInt4C"
      },
      "source": [
        "# Test the model using the test dataset\n",
        "\n",
        "Display the results of the predictions, generate the Confusion Matrix and ROC curve, and measure the AUC, accuracy, precision, and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEPQAPoloCSx"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "\n",
        "results = gbt_model.predict(features_test)\n",
        "print(results)\n",
        "print('\\n')\n",
        "\n",
        "probabilities = gbt_model.predict_proba(features_test)\n",
        "print(probabilities[0:100]) # Display the first 100 sets of probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6AyoNlxou5I"
      },
      "outputs": [],
      "source": [
        "# Generate the confusion matrix from the predictions\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results, display_labels=['Poisonous', 'Edible'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G9d3otK3-Aw"
      },
      "outputs": [],
      "source": [
        "# Calculate the accuracy, precision, and recall\n",
        "# All are a bit low\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results)}')\n",
        "print(f'Recall: {recall_score(predictions_test, results)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYpvM5COoyhh"
      },
      "outputs": [],
      "source": [
        "# Display the ROC curve\n",
        "# Model predictions are not much better than random chance\n",
        "\n",
        "from sklearn.metrics import RocCurveDisplay, auc, roc_curve \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, probabilities[:, 1])\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "display = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name='GBT model', roc_auc=auc_score)\n",
        "display.plot(ax=ax, c='blue')\n",
        "ax.plot((0, 1), (0, 1), c='red', label='Random chance')\n",
        "plt.legend()\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2YomKomZ0l9"
      },
      "outputs": [],
      "source": [
        "# Display the raw data for the ROC curve\n",
        "\n",
        "print(fpr)\n",
        "print('\\n')\n",
        "print(tpr)\n",
        "print('\\n')\n",
        "print(thresholds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkECFtxY5XBq"
      },
      "source": [
        "# Optimize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpkYfn1C5bGw"
      },
      "outputs": [],
      "source": [
        "# Perform a grid search for the best combination of relevant hyperparameters that give the highest precision\n",
        "# NOTE: This step takes about 10 minutes to run. Go and get a cup of tea!\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "hyperparam_values = [\n",
        "  {'n_estimators': [5, 10, 50, 100, 200], 'learning_rate': [0.0, 0.1, 0.5, 3], 'criterion': ['friedman_mse', 'squared_error'], 'max_depth': [2, 3, 5, 10]}\n",
        "]\n",
        "\n",
        "#scoring_metrics = {'AUC': 'roc_auc', 'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall'}\n",
        "scoring_metrics = {'Precision': 'precision'}\n",
        "\n",
        "gbt_gridsearcher = GridSearchCV(GradientBoostingClassifier(), param_grid=hyperparam_values, scoring=scoring_metrics, refit='Precision', return_train_score=True, cv=5)\n",
        "_ = gbt_gridsearcher.fit(features_train, predictions_train)\n",
        "\n",
        "results = gbt_gridsearcher.cv_results_\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEZdwTutHftc"
      },
      "outputs": [],
      "source": [
        "print(gbt_gridsearcher.best_params_)\n",
        "\n",
        "# Capture the best estimator from the grid search\n",
        "precision_gbt_model = gbt_gridsearcher.best_estimator_ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLloje6qX5WC"
      },
      "outputs": [],
      "source": [
        "# Test the best estimator with the test dataset\n",
        "\n",
        "results = precision_gbt_model.predict(features_test)\n",
        "print(results)\n",
        "print('\\n')\n",
        "\n",
        "probabilities = precision_gbt_model.predict_proba(features_test)\n",
        "print(probabilities[0:100]) # Display the first 100 sets of probabilities.Compare with those generated previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni9iKEGLYjlW"
      },
      "outputs": [],
      "source": [
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results, display_labels=['Poisonous', 'Edible'])\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results)}')\n",
        "print(f'Recall: {recall_score(predictions_test, results)}')\n",
        "\n",
        "# Results should be much improved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo_pTxBcY3Ig"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(predictions_test, probabilities[:, 1], drop_intermediate=True)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "display = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name='GBT model', roc_auc=auc_score)\n",
        "display.plot(ax=ax, c='blue')\n",
        "ax.plot((0, 1), (0, 1), c='red', label='Random chance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Model predictions are now much better than random chance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzR4Qeqxciha"
      },
      "outputs": [],
      "source": [
        "print(fpr)\n",
        "print('\\n')\n",
        "print(tpr)\n",
        "print('\\n')\n",
        "print(thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LODwA52W5AqY"
      },
      "outputs": [],
      "source": [
        "# Try again. This time optimize for AUC\n",
        "\n",
        "hyperparam_values = [\n",
        "  {'n_estimators': [5, 10, 50, 100, 200], 'learning_rate': [0.0, 0.1, 0.5, 3], 'criterion': ['friedman_mse', 'squared_error'], 'max_depth': [2, 3, 5, 10]}\n",
        "]\n",
        "\n",
        "scoring_metrics = {'AUC': 'roc_auc'}\n",
        "\n",
        "gbt_gridsearcher = GridSearchCV(GradientBoostingClassifier(), param_grid=hyperparam_values, scoring=scoring_metrics, refit='AUC', return_train_score=True, cv=5)\n",
        "_ = gbt_gridsearcher.fit(features_train, predictions_train)\n",
        "\n",
        "results = gbt_gridsearcher.cv_results_\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0RdoQxC6Csy"
      },
      "outputs": [],
      "source": [
        "print(gbt_gridsearcher.best_params_)\n",
        "auc_gbt_model = gbt_gridsearcher.best_estimator_ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czNFzm5sou2O"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "\n",
        "results = auc_gbt_model.predict(features_test)\n",
        "\n",
        "probabilities = auc_gbt_model.predict_proba(features_test)\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results, display_labels=['Poisonous', 'Edible'])\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results)}')\n",
        "print(f'Recall: {recall_score(predictions_test, results)}')\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, probabilities[:, 1], drop_intermediate=True)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "display = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name='GBT model', roc_auc=auc_score)\n",
        "display.plot(ax=ax, c='blue')\n",
        "ax.plot((0, 1), (0, 1), c='red', label='Random chance')\n",
        "plt.legend()\n",
        "plt.show() \n",
        "\n",
        "# AUC is slightly increased. Precision is slightly reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGOEmhQPeuzL"
      },
      "source": [
        "# Tune to balance variance and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctU5CCwkgmT5"
      },
      "outputs": [],
      "source": [
        "# Search for better value for subsample to balance variance and bias in the precision optimized model\n",
        "# NOTE: This step takes about 5 minutes to run.\n",
        "\n",
        "hyperparam_values = [\n",
        "  {'ccp_alpha': np.arange(0.0, 5.0, 0.5), 'subsample': np.arange(0.1, 1.0, 0.1), 'max_features': ['auto', 'sqrt', 'log2'], 'tol':[1e-1, 1e-2, 1e-4] }\n",
        "]\n",
        "\n",
        "scoring_metrics = {'Precision': 'precision'}\n",
        "\n",
        "gbt_gridsearcher = GridSearchCV(precision_gbt_model, param_grid=hyperparam_values, scoring=scoring_metrics, refit='Precision', return_train_score=True, cv=5)\n",
        "\n",
        "_ = gbt_gridsearcher.fit(features_train, predictions_train)\n",
        "\n",
        "results = gbt_gridsearcher.cv_results_\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HarST8miCr1"
      },
      "outputs": [],
      "source": [
        "print(gbt_gridsearcher.best_params_)\n",
        "gbt_tuned_model = gbt_gridsearcher.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz8T1qXMnHa6"
      },
      "outputs": [],
      "source": [
        "# Test the model with the proposed leaf size\n",
        "\n",
        "results = gbt_tuned_model.predict(features_test)\n",
        "print(results)\n",
        "print('\\n')\n",
        "\n",
        "probabilities = gbt_tuned_model.predict_proba(features_test)\n",
        "print(probabilities[0:100]) # Display the first 100 sets of probabilities.Compare with those generated previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9Zu4PR4naOR"
      },
      "outputs": [],
      "source": [
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results, display_labels=['Poisonous', 'Edible'])\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results)}')\n",
        "print(f'Recall: {recall_score(predictions_test, results)}')\n",
        "\n",
        "# Precision marginally improved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mTrB9hKnkDg"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(predictions_test, probabilities[:, 1], drop_intermediate=True)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "display = RocCurveDisplay(fpr=fpr, tpr=tpr, estimator_name='GBT model', roc_auc=auc_score)\n",
        "display.plot(ax=ax, c='blue')\n",
        "ax.plot((0, 1), (0, 1), c='red', label='Random chance')\n",
        "plt.legend()\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjvRNfW9lQE-"
      },
      "outputs": [],
      "source": [
        "# Plot precision vs subsample\n",
        "\n",
        "# NOTE: Run this several times. The graphs will vary slightly because subsample values less than 1 cause stochastic gradient boosting\n",
        "\n",
        "hyperparam_values = [\n",
        "  {'subsample': np.arange(0.1, 1.0, 0.1)}\n",
        "]\n",
        "\n",
        "scoring_metrics = {'Precision': 'precision', 'Recall': 'recall', 'Accuracy': 'accuracy', 'AUC': 'roc_auc'}\n",
        "\n",
        "gbt_gridsearcher = GridSearchCV(gbt_tuned_model, param_grid=hyperparam_values, scoring=scoring_metrics, refit='Precision', return_train_score=True, cv=5)\n",
        "\n",
        "_ = gbt_gridsearcher.fit(features_train, predictions_train)\n",
        "\n",
        "results = gbt_gridsearcher.cv_results_\n",
        "\n",
        "param_subsample = [r['subsample'] for r in results['params']]\n",
        "\n",
        "mean_test_precision = results['mean_test_Precision']\n",
        "mean_train_precision = results['mean_train_Precision']\n",
        "\n",
        "std_test_precision = results['std_test_Precision']\n",
        "std_train_precision = results['std_train_Precision']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.plot(param_subsample, mean_test_precision, label='Test Precision')\n",
        "plt.fill_between(param_subsample, mean_test_precision - std_test_precision, \\\n",
        "                 mean_test_precision + std_test_precision, alpha=0.1)\n",
        "\n",
        "plt.plot(param_subsample, mean_train_precision, label='Train Precision')\n",
        "plt.fill_between(param_subsample, mean_train_precision - std_train_precision, \\\n",
        "                 mean_train_precision + std_train_precision, alpha=0.1)\n",
        "\n",
        "plt.legend(prop={'size':18})\n",
        "plt.xlabel('Subsample', fontdict={'family':'serif','color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "plt.ylabel('Scores', fontdict={'family': 'serif', 'color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "\n",
        "plt.plot((0.7, 0.7), (0.87, 0.92), c='red')\n",
        "plt.text(0.60, 0.87, 'Subsample=0.7', fontdict={'size': 18})\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p1Q7Xtk2TIy"
      },
      "outputs": [],
      "source": [
        "# Plot recall vs subsample\n",
        "\n",
        "mean_test_recall = results['mean_test_Recall']\n",
        "mean_train_recall = results['mean_train_Recall']\n",
        "\n",
        "std_test_recall = results['std_test_Recall']\n",
        "std_train_recall = results['std_train_Recall']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.plot(param_subsample, mean_test_recall, label='Test Recall')\n",
        "plt.fill_between(param_subsample, mean_test_recall - std_test_recall, \\\n",
        "                 mean_test_recall + std_test_recall, alpha=0.1)\n",
        "\n",
        "plt.plot(param_subsample, mean_train_recall, label='Train Recall')\n",
        "plt.fill_between(param_subsample, mean_train_recall - std_train_recall, \\\n",
        "                 mean_train_recall + std_train_recall, alpha=0.1)\n",
        "\n",
        "plt.legend(prop={'size':18})\n",
        "plt.xlabel('Subsample', fontdict={'family':'serif','color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "plt.ylabel('Scores', fontdict={'family': 'serif', 'color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "\n",
        "plt.plot((0.7, 0.7), (0.87, 0.93), c='red')\n",
        "plt.text(0.60, 0.87, 'Subsample=0.7', fontdict={'size': 18})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNCqBOKN3cB-"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy vs subsample\n",
        "\n",
        "mean_test_accuracy = results['mean_test_Accuracy']\n",
        "mean_train_accuracy = results['mean_train_Accuracy']\n",
        "\n",
        "std_test_accuracy = results['std_test_Accuracy']\n",
        "std_train_accuracy = results['std_train_Accuracy']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.plot(param_subsample, mean_test_accuracy, label='Test Accuracy')\n",
        "plt.fill_between(param_subsample, mean_test_accuracy - std_test_accuracy, \\\n",
        "                 mean_test_accuracy + std_test_accuracy, alpha=0.1)\n",
        "\n",
        "plt.plot(param_subsample, mean_train_accuracy, label='Train Accuracy')\n",
        "plt.fill_between(param_subsample, mean_train_accuracy - std_train_accuracy, \\\n",
        "                 mean_train_accuracy + std_train_accuracy, alpha=0.1)\n",
        "\n",
        "plt.legend(prop={'size':18})\n",
        "plt.xlabel('Subsample', fontdict={'family':'serif','color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "plt.ylabel('Scores', fontdict={'family': 'serif', 'color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "\n",
        "plt.plot((0.7, 0.7), (0.87, 0.93), c='red')\n",
        "plt.text(0.60, 0.87, 'Subsample=0.7', fontdict={'size': 18})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SGoTt914FpS"
      },
      "outputs": [],
      "source": [
        "# Plot AUC vs subsample\n",
        "\n",
        "mean_test_auc = results['mean_test_AUC']\n",
        "mean_train_auc = results['mean_train_AUC']\n",
        "\n",
        "std_test_auc = results['std_test_AUC']\n",
        "std_train_auc = results['std_train_AUC']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.plot(param_subsample, mean_test_auc, label='Test AUC')\n",
        "plt.fill_between(param_subsample, mean_test_auc - std_test_auc, \\\n",
        "                 mean_test_auc + std_test_auc, alpha=0.1)\n",
        "\n",
        "plt.plot(param_subsample, mean_train_auc, label='Train AUC')\n",
        "plt.fill_between(param_subsample, mean_train_auc - std_train_auc, \\\n",
        "                 mean_train_auc + std_train_auc, alpha=0.1)\n",
        "\n",
        "plt.legend(prop={'size':18})\n",
        "plt.xlabel('Subsample', fontdict={'family':'serif','color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "plt.ylabel('Scores', fontdict={'family': 'serif', 'color': 'darkred', \\\n",
        "           'weight': 'normal', 'size': 28})\n",
        "\n",
        "plt.plot((0.7, 0.7), (0.87, 0.94), c='red')\n",
        "plt.text(0.60, 0.87, 'Subsample=0.7', fontdict={'size': 18})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "Setting the correct hyperparameter values can have a significant impact on the performance of the model.\n",
        "\n",
        "The results of tuning to balance bias and variance depend on the data rather than the model. Ideally, tuning should result in small adjustments to the precision, scale, and accuracy. If not, there may be significant skew in the data which should warrant further investigation. For example, is your sampling methodology biased? "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
