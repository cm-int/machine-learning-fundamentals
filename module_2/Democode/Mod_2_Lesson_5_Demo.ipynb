{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_2/Democode/Mod_2_Lesson_5_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Binary and Non-binary Classification\n",
        "\n",
        "In this demonstration, youâ€™ll build and test a multi-class classification model and a multi-label classification model. You will measure the accuracy, precision, recall and AUC for both models.\n",
        "\n",
        "This first part of this demonstration uses and edited version of the Iris dataset. This dataset was used in R.A. Fisher's classic 1936 paper, *The Use of Multiple Measurements in Taxonomic Problems*, and can also be found on the UCI Machine Learning Repository.\n",
        "\n",
        "It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
        "\n",
        "The columns in this dataset are:\n",
        "\n",
        "- SepalLengthCm\n",
        "- SepalWidthCm\n",
        "- PetalLengthCm\n",
        "- PetalWidthCm\n",
        "- Species\n",
        "\n",
        "\n",
        "The second part of this demonstration uses the Emotions dataset [Tsoumakas et al. 2008](https://www.uco.es/kdis/mllresources/#TsoumakasEtAl2008): Also called Music in [Read 2010](https://www.uco.es/kdis/mllresources/#Read2010). This is a small dataset to classify music into emotions that it evokes according to the Tellegen-Watson-Clark model of mood: amazed-suprised, happy-pleased, relaxing-calm, quiet-still, sad-lonely and angry-aggresive. It consists of 593 songs with 6 classes. The same piece of music can cause several emotions.\n",
        "\n",
        "The 72 features in the dataset measure the rhythm and timbre of the music to elicit the emotional responses in the listener."
      ],
      "metadata": {
        "id": "e9eiIWAqmFJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I - Multi-class Classification\n",
        "# Upload and prepare the data\n",
        "\n",
        "Load the Iris dataset"
      ],
      "metadata": {
        "id": "TV5QfJpimVKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the iris.csv file\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_2/Democode/iris.csv'"
      ],
      "metadata": {
        "id": "MFU8yV-T0r5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris = pd.read_csv('iris.csv')\n",
        "print(iris)"
      ],
      "metadata": {
        "id": "PJk4kSe9mmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into test and training datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = iris.drop(['Species'], axis=1)\n",
        "predictions = iris['Species']\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(features, predictions, test_size=0.33, random_state=13) # Random state specified to ensure repeatability if necessary"
      ],
      "metadata": {
        "id": "c9EajuWVvpML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a K-Nearest Neighbors model to classify the data\n",
        "\n",
        "The K-Nearest Neighbors algorithm is inherently multi-class.\n",
        "\n",
        "**Note: You don't need to convert the strings in the Species column into numbers**"
      ],
      "metadata": {
        "id": "HR6r50u0m8kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "_= model.fit(features_train, predictions_train)"
      ],
      "metadata": {
        "id": "-xGa0y9nnCq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model using the test dataset\n",
        "\n",
        "Display the results of the predictions, generate the Confusion Matrix and ROC curve, and measure the AUC, accuracy, precision, and recall"
      ],
      "metadata": {
        "id": "R2SGZtiInt4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "\n",
        "results = model.predict(features_test)\n",
        "print(results) # Note that the predictions have three possible values (there are three species of iris in the dataset)\n",
        "print('\\n')\n",
        "\n",
        "probabilities = model.predict_proba(features_test)\n",
        "print(probabilities[0:100]) # Display the first 100 sets of probabilities. This is a small well-tuned dataset (no noise whatsoever), and we 100% accuracy with this data. \n",
        "                            # Most of the predictions are made with a probability of 1, although several predictions have a probability below this value "
      ],
      "metadata": {
        "id": "DEPQAPoloCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrix from the predictions\n",
        "# A small numer of FPs and FNs\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results)\n"
      ],
      "metadata": {
        "id": "L6AyoNlxou5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize the predictions test data to match the probabilities data\n",
        "\n",
        "# We can't use label_binarize because the labels are strings, not numbers\n",
        "# Instead, we can use pd.get_dummies() and convert the result into a numpy array\n",
        "\n",
        "print(f'Before binarization:\\n{predictions_test}\\n')\n",
        "\n",
        "binarized_predictions_test = np.array(pd.get_dummies(predictions_test))\n",
        "print(f'After binarization:\\n{binarized_predictions_test}')"
      ],
      "metadata": {
        "id": "JeRQw0828TBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy, precision, and recall\n",
        "# All are reasonably high\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results, average=None)}') #Note that you must specify a value for the 'average' parameter with a multi-class model\n",
        "print(f'Recall: {recall_score(predictions_test, results, average=None)}')"
      ],
      "metadata": {
        "id": "-G9d3otK3-Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the ROC curve\n",
        "# Model predictions are ideal, AUC is close to 1.0 for all species\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "num_classes = 3\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(binarized_predictions_test[:, i], probabilities[:, i])\n",
        "    auc = roc_auc_score(binarized_predictions_test[:, i], probabilities[:, i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'class {model.classes_[i]}: AUC {auc}')\n",
        "\n",
        "plt.plot((0, 1), (0,1), label=\"Random Guess\", c='red', linewidth=5)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RYpvM5COoyhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repeat the exercise to create a Logistic Regression model\n",
        "\n",
        "We will use the OvO algorithm, which requires use of the OneVsOne classifier"
      ],
      "metadata": {
        "id": "NkECFtxY5XBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=500))\n",
        "_ = model.fit(features_train, predictions_train)\n",
        "\n",
        "# Display the number of estimators created by the model (there should be three)\n",
        "print(model.estimators_)"
      ],
      "metadata": {
        "id": "IpkYfn1C5bGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make test predictions\n",
        "\n",
        "print(f'Predicted results:\\n{predictions_test}\\n')\n",
        "\n",
        "results = model.predict(features_test)\n",
        "print(f'Actual results:\\n{results}') # Note that the predictions have three possible values (there are three species of iris in the dataset)\n",
        "\n",
        "# Note: The function predict_proba() isn't available for the OneVsOne classifier.\n",
        "# If uncommented, the following statement will cause an error\n",
        "# probabilities = model.predict_proba(features_test)"
      ],
      "metadata": {
        "id": "nEZdwTutHftc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy, precision, and recall\n",
        "# All figures are higher than those of KNN model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(predictions_test, results)}')\n",
        "print(f'Precision: {precision_score(predictions_test, results, average=None)}')\n",
        "print(f'Recall: {recall_score(predictions_test, results, average=None)}')"
      ],
      "metadata": {
        "id": "ZLloje6qX5WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This time we need to binarize the predictions test data and the actual results (they are both currently string data)\n",
        "\n",
        "# Use pd.get_dummies() and convert the result into a numpy array\n",
        "\n",
        "print(f'Test data before binarization:\\n{predictions_test}\\n')\n",
        "\n",
        "binarized_predictions_test = np.array(pd.get_dummies(predictions_test))\n",
        "print(f'Test data after binarization:\\n{binarized_predictions_test}\\n')\n",
        "\n",
        "print(f'Results before binarization:\\n{results}\\n')\n",
        "\n",
        "binarized_results = np.array(pd.get_dummies(results))\n",
        "print(f'Results after binarization:\\n{binarized_results}')"
      ],
      "metadata": {
        "id": "USl1h3XsDRih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, results,)\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(binarized_predictions_test, binarized_results)}')\n",
        "print(f'Precision: {precision_score(binarized_predictions_test, binarized_results, average=None)}')\n",
        "print(f'Recall: {recall_score(binarized_predictions_test, binarized_results, average=None)}')\n",
        "\n",
        "# Results close to 100%"
      ],
      "metadata": {
        "id": "Ni9iKEGLYjlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the ROC curve\n",
        "# AUC is close to 1.0 for all species\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "num_classes = 3\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(binarized_predictions_test[:, i], binarized_results[:, i])\n",
        "    auc = roc_auc_score(binarized_predictions_test[:, i], binarized_results[:, i])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'class {model.classes_[i]}: AUC {auc}')\n",
        "\n",
        "plt.plot((0, 1), (0,1), label=\"Random Guess\", c='red', linewidth=5)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mo_pTxBcY3Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II - Multi-label Classifiction\n",
        "# Upload and prepare the data\n",
        "\n",
        "The Emotions dataset is available for download as part of the Scikit-multilearn library."
      ],
      "metadata": {
        "id": "pGOEmhQPeuzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Install scikit-multilearn and arff (Attribute-Relation File Format)\n",
        " # These are only required to retrieve the Emotions dataset\n",
        "\n",
        "!pip install scikit-multilearn\n",
        "!pip install arff"
      ],
      "metadata": {
        "id": "ctU5CCwkgmT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Emotions dataset. The data has already been split into features and labels\n",
        "import skmultilearn.dataset as sk\n",
        "\n",
        "features, labels, feature_names, label_names = sk.load_dataset('emotions', 'undivided')\n",
        "\n",
        "print(f'Features:\\n{feature_names}\\n\\n')\n",
        "print(f'Labels:\\n{label_names}\\n')"
      ],
      "metadata": {
        "id": "4HarST8miCr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data has to be massaged into a format suitable for use with sklearn classifiers\n",
        "# Create DataFrames for holding features and labels\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd_features = pd.DataFrame(columns=[x[0] for x in feature_names])\n",
        "print(f'{pd_features}\\n')\n",
        "\n",
        "pd_labels = pd.DataFrame(columns=[x[0] for x in label_names])\n",
        "print(f'{pd_labels}\\n')"
      ],
      "metadata": {
        "id": "gaXBD3HmGoQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the data from the sparse array into the two new DataFrames\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i in range(0, np.shape(features)[0]):\n",
        "  new_row=features[i].toarray(order='C')\n",
        "  temp = pd.DataFrame(new_row, columns=[x[0] for x in feature_names])\n",
        "  pd_features = pd_features.append(temp, ignore_index=True).astype('float64')\n",
        "\n",
        "pd_features"
      ],
      "metadata": {
        "id": "RIClU4Z0Kv6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, np.shape(labels)[0]):\n",
        "  new_row=labels[i].toarray(order='C')\n",
        "  temp = pd.DataFrame(new_row, columns=[x[0] for x in label_names])\n",
        "  pd_labels = pd_labels.append(temp, ignore_index=True).astype('float64')\n",
        "\n",
        "pd_labels"
      ],
      "metadata": {
        "id": "D2twUvkJZbWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(pd_features, pd_labels, test_size=0.33, random_state=13)"
      ],
      "metadata": {
        "id": "Qe7Pzy16acZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Multi Output Classifier wrapped around a Gradient Boosted Tree to classify the data\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "multi_model = MultiOutputClassifier(estimator=GradientBoostingClassifier())\n",
        "_ = multi_model.fit(features_train, labels_train)\n",
        "\n",
        "# Show the number of estimators created for the model. There should be one classifier per label (6)\n",
        "print(multi_model.estimators_)"
      ],
      "metadata": {
        "id": "Rz8T1qXMnHa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make test predictions\n",
        "\n",
        "print(f'Predicted results:\\n{labels_test}\\n')\n",
        "\n",
        "results = multi_model.predict(features_test)\n",
        "print(f'Actual results:\\n{results}') # The predictions have up to six labels"
      ],
      "metadata": {
        "id": "Sfh1ShoSdKg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the confusion matrices from the predictions\n",
        "# Note: One confusion matrix per label\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"amazed-suprised\"], results[:, 0])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Amazed-Suprised')\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"happy-pleased\"], results[:, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Happy-Pleased')\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"relaxing-calm\"], results[:, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Relaxing-Calm')\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"quiet-still\"], results[:, 3])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Quiet-Still')\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"sad-lonely\"], results[:, 4])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Sad-Lonely')\n",
        "plt.show()\n",
        "\n",
        "cm = confusion_matrix(labels_test[\"angry-aggresive\"], results[:, 5])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
        "disp.plot()\n",
        "plt.title('Angry-Aggressive')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U0zbATzlfcgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy, precision, and recall\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(labels_test, results)}')\n",
        "print(f'Precision: {precision_score(labels_test, results, average=None)}')\n",
        "print(f'Recall: {recall_score(labels_test, results, average=None)}')\n",
        "\n",
        "# Precision is good, but accuracy is poor"
      ],
      "metadata": {
        "id": "YBO62G3leM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the probabilities for each test prediction.\n",
        "\n",
        "probabilities = multi_model.predict_proba(features_test)\n",
        "print(f'Probabilities: {probabilities}')"
      ],
      "metadata": {
        "id": "5Mk0VmPKdsKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the ROC curves for each label\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "plt.figure(figsize=(10, 10))\n",
        "num_labels=6\n",
        "for i in range(num_labels):\n",
        "    fpr[i], tpr[i], _ = roc_curve(labels_test.iloc[:, i], probabilities[i][:, 1])\n",
        "    auc = roc_auc_score(labels_test.iloc[:, i], probabilities[i][:, 1])\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Label {labels_test.columns[i]}: AUC {auc}')\n",
        "\n",
        "plt.plot((0, 1), (0,1), label=\"Random Guess\", c='red', linewidth=5)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fHWpmDIKetQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}