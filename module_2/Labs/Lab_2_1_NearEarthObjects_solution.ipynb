{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9_7tpMekkVH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cm-int/machine-learning-fundamentals/blob/main/module_2/Labs/Lab_2_1_NearEarthObjects_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaXJHCFLkuTX"
      },
      "source": [
        "# Lab 2.1: Creating a Classification Machine Learning Model\n",
        "\n",
        "In this lab, you’ll build and test several binary classification models to classify Near Earth Objects (NEOs) as Hazardous or Not Hazardous, depending on whether they are likely to collide with the Earth.\n",
        "\n",
        "You’ll build a Decision Tree model, a Random Forest Model, and a Gradient Boosted Tree model. You’ll compare the performance of all three models, and examine the effects of adjusting the probability thresholds of predictions on the recall of a model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZOi1EKE19Kc"
      },
      "source": [
        "**About the data:**\n",
        "\n",
        "(From https://www.kaggle.com/code/elnahas/nasa-nearest-earth-objects/data)\n",
        "\n",
        "There is an infinite number of objects in the outer space. Some of them are closer than we think. Even though we might think that a distance of 70,000 Km can not potentially harm us, but at an astronomical scale, this is a very small distance and can disrupt many natural phenomena. These objects/asteroids can thus prove to be harmful. Hence, it is wise to know what is surrounding us and what can harm us amongst those. This dataset compiles the list of NASA certified asteroids that are classified as the nearest earth object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvs-hwYPFzvo"
      },
      "source": [
        "#Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6EzKobXnRtf"
      },
      "outputs": [],
      "source": [
        "# Upload the neo_v2.csv file from Github\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/cm-int/machine-learning-fundamentals/main/module_2/Labs/neo_v2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyxSttU8F75n"
      },
      "outputs": [],
      "source": [
        "# Read the data from the neo_v2.csv file into a Pandas DataFrame named neos and display the data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "neos = pd.read_csv(\"neo_v2.csv\")\n",
        "print(neos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMPzAUo8KOJE"
      },
      "outputs": [],
      "source": [
        "# We want to predict whether an object is hazardous. Create a new DataFrame named hazardous containing dummy variables for the 'hazardous' column and display the results\n",
        "\n",
        "hazardous = pd.get_dummies(neos.hazardous)\n",
        "print(hazardous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0wNdQ6zCZH"
      },
      "outputs": [],
      "source": [
        "# Rename the two columns to 'Yes' and 'No'. The default names are 'True' and 'False' which are also the names of Python constants, and can cause problems later.\n",
        "\n",
        "hazardous.rename(columns={hazardous.columns[0]: \"No\", hazardous.columns[1]: \"Yes\"}, inplace=True)\n",
        "print(hazardous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJswavAazFUG"
      },
      "outputs": [],
      "source": [
        "# Remove the 'No' column from the DataFrame and convert the result into an array\n",
        "\n",
        "hazardous = hazardous.drop('No', axis=1)\n",
        "hazardous = hazardous.to_numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpfUFmeKzQvb"
      },
      "outputs": [],
      "source": [
        "# Display the first few values from the hazardous array\n",
        "\n",
        "print(hazardous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlfjZis1Lxgz"
      },
      "outputs": [],
      "source": [
        "# Is 'sentry_object' useful in the neo dataframe?  How many different values does it have? (Answer: Just one - False repeated 90836 times)\n",
        "\n",
        "neos.value_counts('sentry_object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68iT-8Xmxkek"
      },
      "outputs": [],
      "source": [
        "# Similarly, is 'orbiting_body' useful?\n",
        "\n",
        "neos.value_counts('orbiting_body') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUHuuyrsKfRc"
      },
      "outputs": [],
      "source": [
        "# Remove the 'hazardous' column from the neos DataFrame, along with columns that probably won't help with classification (orbiting_body, sentry_object)\n",
        "\n",
        "neos = neos.drop(['hazardous', 'orbiting_body', 'sentry_object'], axis=1) \n",
        "neos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veFJUI9dam-T"
      },
      "source": [
        "# Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv40ouqxarIM"
      },
      "outputs": [],
      "source": [
        "# Create a TSNE model and transform the data in the neos array into a 2D representation of the data\n",
        "# NOTE: This step uses a random sample of 2000 rows from the neos array, to save time\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "sample_data = neos.sample(2000)\n",
        "sample_hazardous = hazardous[sample_data.index]\n",
        "\n",
        "visual_model = TSNE(learning_rate = 100, init='pca')\n",
        "visual_transformation = visual_model.fit_transform(sample_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOAFxbuRbBE0"
      },
      "outputs": [],
      "source": [
        "# Extract first and second columns from the array containing the transformed data and use them to create a Pandas DataFrame\n",
        "\n",
        "x_data = visual_transformation[:, 0]\n",
        "y_data = visual_transformation[:, 1]\n",
        "transformed_data = pd.DataFrame({'x':x_data, 'y':y_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNb7jyM2bUet"
      },
      "outputs": [],
      "source": [
        "# Display the results as a Matplotlib graph\n",
        "# The clustering doesn't look promising!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(transformed_data.loc[sample_hazardous==0]['x'], transformed_data.loc[sample_hazardous==0]['y'], c='red')\n",
        "plt.scatter(transformed_data.loc[sample_hazardous==1]['x'], transformed_data.loc[sample_hazardous==1]['y'], c='blue')\n",
        "plt.legend(loc ='lower left', labels = ['Not Hazardous', 'Hazardous'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqwmM696xjcx"
      },
      "outputs": [],
      "source": [
        "# Try creating a 3D TSNE model\n",
        "# NOTE: This step takes between 4 and 5 minutes to run\n",
        "\n",
        "visual_model = TSNE(learning_rate = 100, init='pca', n_components=3)\n",
        "visual_transformation = visual_model.fit_transform(sample_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKlAlcMpxrMk"
      },
      "outputs": [],
      "source": [
        "x_data = visual_transformation[:, 0]\n",
        "y_data = visual_transformation[:, 1]\n",
        "z_data = visual_transformation[:, 2]\n",
        "transformed_data = pd.DataFrame({'x':x_data, 'y':y_data, 'z':z_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HLgh5dVx9DZ"
      },
      "outputs": [],
      "source": [
        "# This time the clustering looks better, so there's a chance we can come up with a decent model\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(transformed_data.loc[sample_hazardous==0]['x'], transformed_data.loc[sample_hazardous==0]['y'], transformed_data.loc[sample_hazardous==0]['z'], c='red')\n",
        "ax.scatter(transformed_data.loc[sample_hazardous==1]['x'], transformed_data.loc[sample_hazardous==1]['y'], transformed_data.loc[sample_hazardous==1]['z'], c='blue')\n",
        "ax.view_init(5, 5)\n",
        "\n",
        "plt.legend(loc ='lower left', labels = ['Not Hazardous', 'Hazardous'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMoAQPxiK8yX"
      },
      "source": [
        "#Classification using a Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruZ7biMlpshi"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, predictions_train, predictions_test = train_test_split(neos, hazardous, test_size=0.33, random_state=13)\n",
        "print(f'features_train: {features_train.shape}\\npredictions_train: {predictions_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moc47aOZLAgb"
      },
      "outputs": [],
      "source": [
        "# Create and fit a Decision Tree model. Use the default values for the hyperparameters.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier_model = DecisionTreeClassifier()\n",
        "_ = classifier_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh4Ib2J9MX7N"
      },
      "outputs": [],
      "source": [
        "# Review the Decision Tree\n",
        "# NOTE: This step takes a minute or two to run. Scroll the results pane to see the tree when it is complete.\n",
        "\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "view_data = export_graphviz(classifier_model, out_file=None, feature_names=neos.columns, class_names=['Not Hazardous', 'Hazardous'])\n",
        "graph = graphviz.Source(view_data)\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfnqZIUuvH0h"
      },
      "outputs": [],
      "source": [
        "# Use the model to make predictions\n",
        "\n",
        "test_results = classifier_model.predict(features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9_YFBOHvTSJ"
      },
      "outputs": [],
      "source": [
        "# Compare the first 30 test predictions to the actual test results\n",
        "\n",
        "print(test_results[0:30])\n",
        "print(predictions_test[0:30])\n",
        "print(f'\\n\\n({(test_results == predictions_test)[0:30]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBKukCm4x5sl"
      },
      "outputs": [],
      "source": [
        "# Examine the accuracy, precision and recall of the model using K-Fold cross-validation. Set K to 5\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring_metrics = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall'}\n",
        "\n",
        "scores = cross_validate(classifier_model, features_train, predictions_train, cv=5, scoring=scoring_metrics)\n",
        "\n",
        "print(f'Accuracy: {scores[\"test_Accuracy\"]}\\n')\n",
        "print(f'Precision: {scores[\"test_Precision\"]}\\n')\n",
        "print(f'Recall: {scores[\"test_Recall\"]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNBIJDL01QmG"
      },
      "outputs": [],
      "source": [
        "# How many potentially Hazardous NEOs have been misclassified as Not Hazardous (false negatives)? Display the confusion matrix for the model.\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=['Not Hazardous', 'Hazardous'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCWtBx5k1pFw"
      },
      "outputs": [],
      "source": [
        "# Generate the ROC curve and calculate the AUC. Is this model better than random guesswork?\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "_ = metrics.RocCurveDisplay.from_predictions(test_results, predictions_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwY_t0mmE-Gj"
      },
      "outputs": [],
      "source": [
        "# Probabilities generated by the Decision Tree Model are either 1 or 0\n",
        "\n",
        "test_probs = classifier_model.predict_proba(features_test)\n",
        "test_probs[0:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_JpR142LLM"
      },
      "source": [
        "# Classification using a Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYC7IqJw2U9V"
      },
      "outputs": [],
      "source": [
        "# Create and fit the model using the same training and test datasets as before\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest_model = RandomForestClassifier(n_estimators=100) \n",
        "_ = forest_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnbCMCjM349R"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "\n",
        "test_results = forest_model.predict(features_test)\n",
        "print(test_results[0:30])\n",
        "print(predictions_test[0:30])\n",
        "print(f'\\n\\n({(test_results == predictions_test)[0:30]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HdvBzrt8_WH"
      },
      "outputs": [],
      "source": [
        "# Cross-validate the model and compare the accuracy, precision, and recall against the Decision Tree model\n",
        "\n",
        "scoring_metrics = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall'}\n",
        "scores = cross_validate(forest_model, features_train, predictions_train, cv=5, scoring=scoring_metrics)\n",
        "\n",
        "print(f'Accuracy: {scores[\"test_Accuracy\"]}\\n')\n",
        "print(f'Precision: {scores[\"test_Precision\"]}\\n')\n",
        "print(f'Recall: {scores[\"test_Recall\"]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M604mrp5_Zte"
      },
      "outputs": [],
      "source": [
        "# Compare Confusion Matrix and ROC curves to the Decision Tree Model\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=['Not Hazardous', 'Hazardous'])\n",
        "_ = metrics.RocCurveDisplay.from_estimator(forest_model, features_test, predictions_test)\n",
        "_ = metrics.RocCurveDisplay.from_predictions(test_results, predictions_test)\n",
        "\n",
        "# Note: The number of FP and FNs have both dropped slightly. The model is still more likely to label a Hazardous NEO as Not Hazardous.\n",
        "# Is this a good thing? The default threshold of 0.5 is probably too high for this dataset, so investigate further ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWXtQhj2FLML"
      },
      "outputs": [],
      "source": [
        "# Find the probabilities for each prediction in the test sample\n",
        "\n",
        "test_probs = forest_model.predict_proba(features_test)\n",
        "test_probs[1:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3NjdE10FvHf"
      },
      "outputs": [],
      "source": [
        "# Keep the probabilities for the positive outcome (Hazardous) and discard those for the negative outcome (Not Hazardous)\n",
        "\n",
        "test_probs_hazardous = test_probs[:, 1]\n",
        "print(f'{test_probs_hazardous[1:30]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7nbPLn2GGbx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, test_probs_hazardous)\n",
        "\n",
        "# Calculate the J statistic to find the optimal threshold for this model\n",
        "J = tpr - fpr\n",
        "idx = np.argmax(J)\n",
        "optimal_threshold = thresholds[idx]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot([0,1], [0,1], linestyle='--', label='Random')\n",
        "plt.plot(fpr, tpr, marker='.', markersize=5, label='Model')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.scatter(fpr[idx], tpr[idx], marker='X', s=200, color='Blue', label=f'Optimal threshold at P={optimal_threshold}')\n",
        "plt.legend()\n",
        "_ = plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EYuH34UtEP"
      },
      "outputs": [],
      "source": [
        "print(f'Predictions with a probability >= {optimal_threshold} should be classed as Hazardous')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h9M7wrAs7eo"
      },
      "outputs": [],
      "source": [
        "# Examine how using the optimal threshold impacts the quality of the predictions made using the model \n",
        "\n",
        "# Find the index for every prediction with a probability probability >= optimal_threshold\n",
        "indexes = np.where(test_probs_hazardous >= optimal_threshold)\n",
        "\n",
        "# Create a new array cor holding the adjusted predictions after applying the new threshold. Initialize it with zeros, and make it the same length as the original set of test results\n",
        "adjusted_test_results = np.zeros(test_results.size)\n",
        "\n",
        "# Set the value in the adjusted predictions array to 1 for each item indicated by the indexes array\n",
        "adjusted_test_results[indexes] = 1\n",
        "\n",
        "# Display the results\n",
        "print(adjusted_test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9C-KMEK51P2"
      },
      "outputs": [],
      "source": [
        "# Generate a confusion matrix comparing the predictions test data and the adjusted test results\n",
        "\n",
        "# The number of false negatives should have dropped considerably, although there is now an increased level of false positives. Are false positives better than false negatives?\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, adjusted_test_results, display_labels=['Not Hazardous', 'Hazardous'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8dR1aFgLEaK"
      },
      "outputs": [],
      "source": [
        "# Calculate the accuracy, precision, and recall for the model using this new threshold\n",
        "# Recall should be much improved from before. Why is this more important than precision for this model?\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "accuracy = accuracy_score(predictions_test, adjusted_test_results)\n",
        "precision = precision_score(predictions_test, adjusted_test_results)\n",
        "recall = recall_score(predictions_test, adjusted_test_results)\n",
        "\n",
        "print(f'Accuracy: {accuracy}\\n')\n",
        "print(f'Precision: {precision}\\n')\n",
        "print(f'Recall: {recall}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMxSIm_UP_WX"
      },
      "source": [
        "#Classification using Gradient Boosted Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUYCSh3SQYGh"
      },
      "outputs": [],
      "source": [
        "# Create and fit the model\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbt_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "_ = gbt_model.fit(features_train, predictions_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ZgmH31RAJt"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "\n",
        "test_results = gbt_model.predict(features_test)\n",
        "print(f'{(test_results == predictions_test)[0:30]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtcCD5RMRNpw"
      },
      "outputs": [],
      "source": [
        "# Perform cross-validation, and calculate the accuracy, precision and recall of the model\n",
        "\n",
        "scoring_metrics = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall'}\n",
        "scores = cross_validate(gbt_model, features_train, predictions_train, cv=5, scoring=scoring_metrics)\n",
        "\n",
        "print(f'Accuracy: {scores[\"test_Accuracy\"]}\\n')\n",
        "print(f'Precision: {scores[\"test_Precision\"]}\\n')\n",
        "print(f'Recall: {scores[\"test_Recall\"]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgfEJkoHR8j-"
      },
      "outputs": [],
      "source": [
        "# Generate the Confusion Matrix and ROC curves\n",
        "\n",
        "_ = ConfusionMatrixDisplay.from_predictions(predictions_test, test_results, display_labels=['Not Hazardous', 'Hazardous'])\n",
        "_ = metrics.RocCurveDisplay.from_estimator(gbt_model, features_test, predictions_test)\n",
        "\n",
        "# This model has even more FNs than the original Random Forest, but far fewer FPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwo1FUFHUAxX"
      },
      "outputs": [],
      "source": [
        "# Calculate the optimal threshold for this model, as per the Random Forest model\n",
        "\n",
        "test_probs = gbt_model.predict_proba(features_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(predictions_test, test_probs)\n",
        "\n",
        "# Calculate the J statistic to find the optimal threshold for this model\n",
        "J = tpr - fpr\n",
        "idx = np.argmax(J)\n",
        "optimal_threshold = thresholds[idx]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot([0,1], [0,1], linestyle='--', label='Random')\n",
        "plt.plot(fpr, tpr, marker='.', markersize=5, label='Model')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.scatter(fpr[idx], tpr[idx], marker='X', s=200, color='Blue', label=f'Optimal threshold at P={optimal_threshold}')\n",
        "plt.legend()\n",
        "_ = plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMooTsG8Unx1"
      },
      "outputs": [],
      "source": [
        "print(f'Predictions with a probability >= {optimal_threshold} should be classed as Hazardous')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TvnMRMtVkpg"
      },
      "outputs": [],
      "source": [
        "# Follow the same process as before to see how amending the threshold affects the quality of the model\n",
        "\n",
        "# Find the index for every prediction with a probability probability >= optimal_threshold\n",
        "indexes = np.where(test_probs_hazardous >= optimal_threshold)\n",
        "\n",
        "# Create a new array cor holding the adjusted predictions after applying the new threshold. Initialize it with zeros, and make it the same length as the original set of test results\n",
        "adjusted_test_results = np.zeros(test_results.size)\n",
        "\n",
        "# Set the value in the adjusted predictions array to 1 for each item indicated by the indexes array\n",
        "adjusted_test_results[indexes] = 1\n",
        "\n",
        "# Display the results\n",
        "print(adjusted_test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJqsm0ZVV-i3"
      },
      "outputs": [],
      "source": [
        "# Calculate the accuracy, precision, and recall for the model using this new threshold\n",
        "# Again, recall should be much improved from before.\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "accuracy = accuracy_score(predictions_test, adjusted_test_results)\n",
        "precision = precision_score(predictions_test, adjusted_test_results)\n",
        "recall = recall_score(predictions_test, adjusted_test_results)\n",
        "\n",
        "print(f'Accuracy: {accuracy}\\n')\n",
        "print(f'Precision: {precision}\\n')\n",
        "print(f'Recall: {recall}\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
